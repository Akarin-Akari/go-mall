# é«˜çº§ç¯‡ç¬¬å››ç« ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€å·§ ğŸš€

> *"æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€é—¨è‰ºæœ¯ï¼Œéœ€è¦åœ¨åŠŸèƒ½ã€æ€§èƒ½ã€å¯ç»´æŠ¤æ€§ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ã€‚ä¼˜åŒ–ä¸æ˜¯é“¶å¼¹ï¼Œè€Œæ˜¯åŸºäºæ•°æ®é©±åŠ¨çš„ç§‘å­¦å†³ç­–è¿‡ç¨‹ï¼"* âš¡

## ğŸ“š æœ¬ç« å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†æŒæ¡ï¼š

- ğŸ” **Goç¨‹åºæ€§èƒ½åˆ†æå·¥å…·**ï¼šæ·±å…¥æŒæ¡pprofã€traceã€go toolç­‰æ€§èƒ½åˆ†æåˆ©å™¨
- ğŸ§  **å†…å­˜ä¼˜åŒ–ç­–ç•¥**ï¼šå†…å­˜æ³„æ¼æ£€æµ‹ã€GCè°ƒä¼˜ã€å¯¹è±¡æ± ç­‰å†…å­˜ç®¡ç†æŠ€å·§
- âš¡ **å¹¶å‘æ€§èƒ½è°ƒä¼˜**ï¼šgoroutineæ± ã€channelä¼˜åŒ–ã€é”ä¼˜åŒ–ç­‰å¹¶å‘ç¼–ç¨‹ä¼˜åŒ–
- ğŸ—„ï¸ **æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–**ï¼šè¿æ¥æ± ã€æŸ¥è¯¢ä¼˜åŒ–ã€ç´¢å¼•è®¾è®¡ç­‰æ•°æ®åº“ä¼˜åŒ–ç­–ç•¥
- ğŸ’¾ **ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**ï¼šå¤šçº§ç¼“å­˜ã€ç¼“å­˜ç©¿é€/é›ªå´©é˜²æŠ¤ç­‰ç¼“å­˜ä¼˜åŒ–æŠ€æœ¯
- ğŸŒ **ç½‘ç»œæ€§èƒ½ä¼˜åŒ–**ï¼šHTTP/2ã€è¿æ¥å¤ç”¨ã€å‹ç¼©ç­‰ç½‘ç»œå±‚ä¼˜åŒ–
- ğŸ¢ **Mall-Goæ€§èƒ½ä¼˜åŒ–å®æˆ˜**ï¼šç»“åˆç”µå•†é¡¹ç›®çš„å®Œæ•´æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹

---

## ğŸ” Goç¨‹åºæ€§èƒ½åˆ†æå·¥å…·è¯¦è§£

### pprofæ€§èƒ½åˆ†æ

pprofæ˜¯Goè¯­è¨€å†…ç½®çš„æ€§èƒ½åˆ†æå·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬è¯†åˆ«ç¨‹åºçš„æ€§èƒ½ç“¶é¢ˆã€‚

```go
// æ€§èƒ½åˆ†æå·¥å…·é›†æˆ
package profiling

import (
    "context"
    "fmt"
    "net/http"
    _ "net/http/pprof"
    "os"
    "runtime"
    "runtime/pprof"
    "runtime/trace"
    "time"
    
    "github.com/gin-gonic/gin"
    "github.com/pkg/profile"
)

// æ€§èƒ½åˆ†æç®¡ç†å™¨
type ProfilingManager struct {
    enabled     bool
    profileType string
    outputDir   string
    server      *http.Server
}

// æ€§èƒ½åˆ†æé…ç½®
type ProfilingConfig struct {
    Enabled     bool   `json:"enabled"`
    Port        int    `json:"port"`
    ProfileType string `json:"profile_type"` // cpu, mem, block, mutex, goroutine, trace
    OutputDir   string `json:"output_dir"`
    Duration    int    `json:"duration"`     // åˆ†ææŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
}

// åˆ›å»ºæ€§èƒ½åˆ†æç®¡ç†å™¨
func NewProfilingManager(config *ProfilingConfig) *ProfilingManager {
    return &ProfilingManager{
        enabled:     config.Enabled,
        profileType: config.ProfileType,
        outputDir:   config.OutputDir,
    }
}

// å¯åŠ¨æ€§èƒ½åˆ†ææœåŠ¡
func (pm *ProfilingManager) StartProfilingServer(port int) error {
    if !pm.enabled {
        return nil
    }
    
    mux := http.NewServeMux()
    
    // æ³¨å†Œpprofè·¯ç”±
    mux.HandleFunc("/debug/pprof/", http.HandlerFunc(pprof.Index))
    mux.HandleFunc("/debug/pprof/cmdline", http.HandlerFunc(pprof.Cmdline))
    mux.HandleFunc("/debug/pprof/profile", http.HandlerFunc(pprof.Profile))
    mux.HandleFunc("/debug/pprof/symbol", http.HandlerFunc(pprof.Symbol))
    mux.HandleFunc("/debug/pprof/trace", http.HandlerFunc(pprof.Trace))
    
    // è‡ªå®šä¹‰åˆ†æç«¯ç‚¹
    mux.HandleFunc("/debug/pprof/heap", pm.heapProfile)
    mux.HandleFunc("/debug/pprof/goroutine", pm.goroutineProfile)
    mux.HandleFunc("/debug/pprof/block", pm.blockProfile)
    mux.HandleFunc("/debug/pprof/mutex", pm.mutexProfile)
    
    // æ€§èƒ½åˆ†ææ§åˆ¶ç«¯ç‚¹
    mux.HandleFunc("/profiling/start", pm.startProfiling)
    mux.HandleFunc("/profiling/stop", pm.stopProfiling)
    mux.HandleFunc("/profiling/status", pm.profilingStatus)
    
    pm.server = &http.Server{
        Addr:    fmt.Sprintf(":%d", port),
        Handler: mux,
    }
    
    go func() {
        if err := pm.server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            fmt.Printf("Profiling server error: %v\n", err)
        }
    }()
    
    fmt.Printf("Profiling server started on port %d\n", port)
    return nil
}

// å †å†…å­˜åˆ†æ
func (pm *ProfilingManager) heapProfile(w http.ResponseWriter, r *http.Request) {
    runtime.GC() // å¼ºåˆ¶GCï¼Œè·å–å‡†ç¡®çš„å †ä¿¡æ¯
    pprof.WriteHeapProfile(w)
}

// Goroutineåˆ†æ
func (pm *ProfilingManager) goroutineProfile(w http.ResponseWriter, r *http.Request) {
    pprof.Lookup("goroutine").WriteTo(w, 1)
}

// é˜»å¡åˆ†æ
func (pm *ProfilingManager) blockProfile(w http.ResponseWriter, r *http.Request) {
    pprof.Lookup("block").WriteTo(w, 1)
}

// äº’æ–¥é”åˆ†æ
func (pm *ProfilingManager) mutexProfile(w http.ResponseWriter, r *http.Request) {
    pprof.Lookup("mutex").WriteTo(w, 1)
}

// å¼€å§‹æ€§èƒ½åˆ†æ
func (pm *ProfilingManager) startProfiling(w http.ResponseWriter, r *http.Request) {
    profileType := r.URL.Query().Get("type")
    if profileType == "" {
        profileType = "cpu"
    }
    
    duration := 30 * time.Second
    if d := r.URL.Query().Get("duration"); d != "" {
        if parsed, err := time.ParseDuration(d); err == nil {
            duration = parsed
        }
    }
    
    switch profileType {
    case "cpu":
        pm.startCPUProfiling(duration)
    case "mem":
        pm.startMemoryProfiling(duration)
    case "trace":
        pm.startTraceProfiling(duration)
    default:
        http.Error(w, "Unsupported profile type", http.StatusBadRequest)
        return
    }
    
    w.WriteHeader(http.StatusOK)
    fmt.Fprintf(w, "Started %s profiling for %v\n", profileType, duration)
}

// CPUæ€§èƒ½åˆ†æ
func (pm *ProfilingManager) startCPUProfiling(duration time.Duration) {
    filename := fmt.Sprintf("%s/cpu_profile_%d.prof", pm.outputDir, time.Now().Unix())
    f, err := os.Create(filename)
    if err != nil {
        fmt.Printf("Failed to create CPU profile file: %v\n", err)
        return
    }
    
    if err := pprof.StartCPUProfile(f); err != nil {
        fmt.Printf("Failed to start CPU profile: %v\n", err)
        f.Close()
        return
    }
    
    go func() {
        time.Sleep(duration)
        pprof.StopCPUProfile()
        f.Close()
        fmt.Printf("CPU profile saved to %s\n", filename)
    }()
}

// å†…å­˜æ€§èƒ½åˆ†æ
func (pm *ProfilingManager) startMemoryProfiling(duration time.Duration) {
    go func() {
        time.Sleep(duration)
        filename := fmt.Sprintf("%s/mem_profile_%d.prof", pm.outputDir, time.Now().Unix())
        f, err := os.Create(filename)
        if err != nil {
            fmt.Printf("Failed to create memory profile file: %v\n", err)
            return
        }
        defer f.Close()
        
        runtime.GC() // å¼ºåˆ¶GC
        if err := pprof.WriteHeapProfile(f); err != nil {
            fmt.Printf("Failed to write memory profile: %v\n", err)
            return
        }
        
        fmt.Printf("Memory profile saved to %s\n", filename)
    }()
}

// æ‰§è¡Œè¿½è¸ªåˆ†æ
func (pm *ProfilingManager) startTraceProfiling(duration time.Duration) {
    filename := fmt.Sprintf("%s/trace_%d.out", pm.outputDir, time.Now().Unix())
    f, err := os.Create(filename)
    if err != nil {
        fmt.Printf("Failed to create trace file: %v\n", err)
        return
    }
    
    if err := trace.Start(f); err != nil {
        fmt.Printf("Failed to start trace: %v\n", err)
        f.Close()
        return
    }
    
    go func() {
        time.Sleep(duration)
        trace.Stop()
        f.Close()
        fmt.Printf("Trace saved to %s\n", filename)
    }()
}

// åœæ­¢æ€§èƒ½åˆ†æ
func (pm *ProfilingManager) stopProfiling(w http.ResponseWriter, r *http.Request) {
    pprof.StopCPUProfile()
    trace.Stop()
    w.WriteHeader(http.StatusOK)
    fmt.Fprint(w, "Profiling stopped\n")
}

// æ€§èƒ½åˆ†æçŠ¶æ€
func (pm *ProfilingManager) profilingStatus(w http.ResponseWriter, r *http.Request) {
    status := map[string]interface{}{
        "enabled":     pm.enabled,
        "goroutines":  runtime.NumGoroutine(),
        "memory":      getMemoryStats(),
        "gc_stats":    getGCStats(),
    }
    
    w.Header().Set("Content-Type", "application/json")
    fmt.Fprintf(w, "%+v\n", status)
}

// è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯
func getMemoryStats() map[string]interface{} {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    
    return map[string]interface{}{
        "alloc":         m.Alloc,
        "total_alloc":   m.TotalAlloc,
        "sys":           m.Sys,
        "heap_alloc":    m.HeapAlloc,
        "heap_sys":      m.HeapSys,
        "heap_idle":     m.HeapIdle,
        "heap_inuse":    m.HeapInuse,
        "heap_objects":  m.HeapObjects,
        "stack_inuse":   m.StackInuse,
        "stack_sys":     m.StackSys,
        "next_gc":       m.NextGC,
        "last_gc":       m.LastGC,
        "num_gc":        m.NumGC,
        "gc_cpu_fraction": m.GCCPUFraction,
    }
}

// è·å–GCç»Ÿè®¡ä¿¡æ¯
func getGCStats() map[string]interface{} {
    var stats runtime.GCStats
    runtime.ReadGCStats(&stats)
    
    return map[string]interface{}{
        "last_gc":       stats.LastGC,
        "num_gc":        stats.NumGC,
        "pause_total":   stats.PauseTotal,
        "pause_ns":      stats.PauseNs,
        "pause_end":     stats.PauseEnd,
        "pause_quantiles": stats.PauseQuantiles,
    }
}

// Ginä¸­é—´ä»¶ - æ€§èƒ½ç›‘æ§
func (pm *ProfilingManager) PerformanceMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        if !pm.enabled {
            c.Next()
            return
        }
        
        start := time.Now()
        
        // è®°å½•è¯·æ±‚å¼€å§‹æ—¶çš„èµ„æºçŠ¶æ€
        var startMem runtime.MemStats
        runtime.ReadMemStats(&startMem)
        startGoroutines := runtime.NumGoroutine()
        
        // å¤„ç†è¯·æ±‚
        c.Next()
        
        // è®°å½•è¯·æ±‚ç»“æŸæ—¶çš„èµ„æºçŠ¶æ€
        var endMem runtime.MemStats
        runtime.ReadMemStats(&endMem)
        endGoroutines := runtime.NumGoroutine()
        
        duration := time.Since(start)
        
        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        c.Header("X-Response-Time", duration.String())
        c.Header("X-Memory-Alloc", fmt.Sprintf("%d", endMem.Alloc-startMem.Alloc))
        c.Header("X-Goroutines", fmt.Sprintf("%d", endGoroutines))
        c.Header("X-Goroutine-Delta", fmt.Sprintf("%d", endGoroutines-startGoroutines))
        
        // å¦‚æœè¯·æ±‚æ—¶é—´è¿‡é•¿ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        if duration > 1*time.Second {
            fmt.Printf("Slow request detected: %s %s took %v\n", 
                c.Request.Method, c.Request.URL.Path, duration)
        }
    }
}

// è‡ªåŠ¨æ€§èƒ½åˆ†æå™¨
type AutoProfiler struct {
    manager     *ProfilingManager
    thresholds  *PerformanceThresholds
    enabled     bool
    lastProfile time.Time
}

// æ€§èƒ½é˜ˆå€¼é…ç½®
type PerformanceThresholds struct {
    CPUUsage        float64       `json:"cpu_usage"`         // CPUä½¿ç”¨ç‡é˜ˆå€¼
    MemoryUsage     uint64        `json:"memory_usage"`      // å†…å­˜ä½¿ç”¨é‡é˜ˆå€¼
    GoroutineCount  int           `json:"goroutine_count"`   // Goroutineæ•°é‡é˜ˆå€¼
    GCPause         time.Duration `json:"gc_pause"`          // GCæš‚åœæ—¶é—´é˜ˆå€¼
    ResponseTime    time.Duration `json:"response_time"`     // å“åº”æ—¶é—´é˜ˆå€¼
    ProfileInterval time.Duration `json:"profile_interval"`  // è‡ªåŠ¨åˆ†æé—´éš”
}

// åˆ›å»ºè‡ªåŠ¨æ€§èƒ½åˆ†æå™¨
func NewAutoProfiler(manager *ProfilingManager, thresholds *PerformanceThresholds) *AutoProfiler {
    return &AutoProfiler{
        manager:    manager,
        thresholds: thresholds,
        enabled:    true,
    }
}

// å¯åŠ¨è‡ªåŠ¨æ€§èƒ½åˆ†æ
func (ap *AutoProfiler) Start(ctx context.Context) {
    if !ap.enabled {
        return
    }
    
    ticker := time.NewTicker(30 * time.Second) // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            ap.checkAndProfile()
        }
    }
}

// æ£€æŸ¥å¹¶æ‰§è¡Œæ€§èƒ½åˆ†æ
func (ap *AutoProfiler) checkAndProfile() {
    // æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæ€§èƒ½åˆ†æ
    if time.Since(ap.lastProfile) < ap.thresholds.ProfileInterval {
        return
    }
    
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    
    goroutineCount := runtime.NumGoroutine()
    
    // æ£€æŸ¥å„é¡¹æŒ‡æ ‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼
    needProfile := false
    profileType := "cpu"
    
    if m.Alloc > ap.thresholds.MemoryUsage {
        needProfile = true
        profileType = "mem"
        fmt.Printf("Memory usage threshold exceeded: %d > %d\n", m.Alloc, ap.thresholds.MemoryUsage)
    }
    
    if goroutineCount > ap.thresholds.GoroutineCount {
        needProfile = true
        profileType = "goroutine"
        fmt.Printf("Goroutine count threshold exceeded: %d > %d\n", goroutineCount, ap.thresholds.GoroutineCount)
    }
    
    if m.PauseNs[(m.NumGC+255)%256] > uint64(ap.thresholds.GCPause) {
        needProfile = true
        profileType = "cpu"
        fmt.Printf("GC pause threshold exceeded: %d > %d\n", 
            m.PauseNs[(m.NumGC+255)%256], ap.thresholds.GCPause)
    }
    
    if needProfile {
        ap.triggerProfiling(profileType)
        ap.lastProfile = time.Now()
    }
}

// è§¦å‘æ€§èƒ½åˆ†æ
func (ap *AutoProfiler) triggerProfiling(profileType string) {
    fmt.Printf("Auto-triggering %s profiling due to threshold breach\n", profileType)
    
    switch profileType {
    case "cpu":
        ap.manager.startCPUProfiling(30 * time.Second)
    case "mem":
        ap.manager.startMemoryProfiling(10 * time.Second)
    case "trace":
        ap.manager.startTraceProfiling(15 * time.Second)
    }
}
```

---

## ğŸ§  å†…å­˜ä¼˜åŒ–ç­–ç•¥

### å†…å­˜æ³„æ¼æ£€æµ‹

å†…å­˜æ³„æ¼æ˜¯Goç¨‹åºæ€§èƒ½é—®é¢˜çš„å¸¸è§åŸå› ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„æ£€æµ‹å’Œé¢„é˜²æœºåˆ¶ã€‚

```go
// å†…å­˜æ³„æ¼æ£€æµ‹å™¨
package memory

import (
    "context"
    "fmt"
    "runtime"
    "sync"
    "time"
)

// å†…å­˜æ³„æ¼æ£€æµ‹å™¨
type MemoryLeakDetector struct {
    enabled         bool
    checkInterval   time.Duration
    thresholds      *MemoryThresholds
    history         []MemorySnapshot
    historyMutex    sync.RWMutex
    maxHistorySize  int
    alertCallback   func(alert *MemoryAlert)
}

// å†…å­˜é˜ˆå€¼é…ç½®
type MemoryThresholds struct {
    HeapGrowthRate    float64 `json:"heap_growth_rate"`    // å †å¢é•¿ç‡é˜ˆå€¼ (%)
    GoroutineGrowth   int     `json:"goroutine_growth"`    // Goroutineå¢é•¿é˜ˆå€¼
    GCFrequency       int     `json:"gc_frequency"`        // GCé¢‘ç‡é˜ˆå€¼ (æ¬¡/åˆ†é’Ÿ)
    MemoryUsageLimit  uint64  `json:"memory_usage_limit"`  // å†…å­˜ä½¿ç”¨é™åˆ¶ (bytes)
    LeakDetectionTime time.Duration `json:"leak_detection_time"` // æ³„æ¼æ£€æµ‹æ—¶é—´çª—å£
}

// å†…å­˜å¿«ç…§
type MemorySnapshot struct {
    Timestamp       time.Time `json:"timestamp"`
    HeapAlloc       uint64    `json:"heap_alloc"`
    HeapSys         uint64    `json:"heap_sys"`
    HeapIdle        uint64    `json:"heap_idle"`
    HeapInuse       uint64    `json:"heap_inuse"`
    HeapObjects     uint64    `json:"heap_objects"`
    GoroutineCount  int       `json:"goroutine_count"`
    NumGC           uint32    `json:"num_gc"`
    GCCPUFraction   float64   `json:"gc_cpu_fraction"`
    NextGC          uint64    `json:"next_gc"`
    LastGC          uint64    `json:"last_gc"`
}

// å†…å­˜å‘Šè­¦
type MemoryAlert struct {
    Type        string    `json:"type"`
    Severity    string    `json:"severity"`
    Message     string    `json:"message"`
    Timestamp   time.Time `json:"timestamp"`
    CurrentMem  uint64    `json:"current_mem"`
    PreviousMem uint64    `json:"previous_mem"`
    GrowthRate  float64   `json:"growth_rate"`
    Suggestions []string  `json:"suggestions"`
}

// åˆ›å»ºå†…å­˜æ³„æ¼æ£€æµ‹å™¨
func NewMemoryLeakDetector(thresholds *MemoryThresholds, alertCallback func(*MemoryAlert)) *MemoryLeakDetector {
    return &MemoryLeakDetector{
        enabled:        true,
        checkInterval:  30 * time.Second,
        thresholds:     thresholds,
        history:        make([]MemorySnapshot, 0),
        maxHistorySize: 100,
        alertCallback:  alertCallback,
    }
}

// å¯åŠ¨å†…å­˜ç›‘æ§
func (mld *MemoryLeakDetector) Start(ctx context.Context) {
    if !mld.enabled {
        return
    }

    ticker := time.NewTicker(mld.checkInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            mld.checkMemoryUsage()
        }
    }
}

// æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
func (mld *MemoryLeakDetector) checkMemoryUsage() {
    snapshot := mld.takeSnapshot()
    mld.addSnapshot(snapshot)

    // åˆ†æå†…å­˜è¶‹åŠ¿
    if len(mld.history) >= 2 {
        mld.analyzeMemoryTrend()
    }

    // æ£€æŸ¥å†…å­˜æ³„æ¼
    if len(mld.history) >= 5 {
        mld.detectMemoryLeak()
    }
}

// è·å–å†…å­˜å¿«ç…§
func (mld *MemoryLeakDetector) takeSnapshot() MemorySnapshot {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)

    return MemorySnapshot{
        Timestamp:      time.Now(),
        HeapAlloc:      m.Alloc,
        HeapSys:        m.Sys,
        HeapIdle:       m.HeapIdle,
        HeapInuse:      m.HeapInuse,
        HeapObjects:    m.HeapObjects,
        GoroutineCount: runtime.NumGoroutine(),
        NumGC:          m.NumGC,
        GCCPUFraction:  m.GCCPUFraction,
        NextGC:         m.NextGC,
        LastGC:         m.LastGC,
    }
}

// æ·»åŠ å¿«ç…§åˆ°å†å²è®°å½•
func (mld *MemoryLeakDetector) addSnapshot(snapshot MemorySnapshot) {
    mld.historyMutex.Lock()
    defer mld.historyMutex.Unlock()

    mld.history = append(mld.history, snapshot)

    // ä¿æŒå†å²è®°å½•å¤§å°é™åˆ¶
    if len(mld.history) > mld.maxHistorySize {
        mld.history = mld.history[1:]
    }
}

// åˆ†æå†…å­˜è¶‹åŠ¿
func (mld *MemoryLeakDetector) analyzeMemoryTrend() {
    mld.historyMutex.RLock()
    defer mld.historyMutex.RUnlock()

    if len(mld.history) < 2 {
        return
    }

    current := mld.history[len(mld.history)-1]
    previous := mld.history[len(mld.history)-2]

    // è®¡ç®—å†…å­˜å¢é•¿ç‡
    heapGrowthRate := float64(current.HeapAlloc-previous.HeapAlloc) / float64(previous.HeapAlloc) * 100
    goroutineGrowth := current.GoroutineCount - previous.GoroutineCount

    // æ£€æŸ¥å †å†…å­˜å¢é•¿
    if heapGrowthRate > mld.thresholds.HeapGrowthRate {
        alert := &MemoryAlert{
            Type:        "heap_growth",
            Severity:    "warning",
            Message:     fmt.Sprintf("Heap memory growth rate %.2f%% exceeds threshold %.2f%%", heapGrowthRate, mld.thresholds.HeapGrowthRate),
            Timestamp:   time.Now(),
            CurrentMem:  current.HeapAlloc,
            PreviousMem: previous.HeapAlloc,
            GrowthRate:  heapGrowthRate,
            Suggestions: []string{
                "Check for memory leaks in recent code changes",
                "Review object lifecycle management",
                "Consider using object pools for frequently allocated objects",
            },
        }
        mld.sendAlert(alert)
    }

    // æ£€æŸ¥Goroutineå¢é•¿
    if goroutineGrowth > mld.thresholds.GoroutineGrowth {
        alert := &MemoryAlert{
            Type:      "goroutine_growth",
            Severity:  "warning",
            Message:   fmt.Sprintf("Goroutine count increased by %d, exceeds threshold %d", goroutineGrowth, mld.thresholds.GoroutineGrowth),
            Timestamp: time.Now(),
            Suggestions: []string{
                "Check for goroutine leaks",
                "Ensure all goroutines have proper exit conditions",
                "Review context cancellation usage",
            },
        }
        mld.sendAlert(alert)
    }

    // æ£€æŸ¥å†…å­˜ä½¿ç”¨é™åˆ¶
    if current.HeapAlloc > mld.thresholds.MemoryUsageLimit {
        alert := &MemoryAlert{
            Type:       "memory_limit",
            Severity:   "critical",
            Message:    fmt.Sprintf("Memory usage %d bytes exceeds limit %d bytes", current.HeapAlloc, mld.thresholds.MemoryUsageLimit),
            Timestamp:  time.Now(),
            CurrentMem: current.HeapAlloc,
            Suggestions: []string{
                "Immediate memory cleanup required",
                "Consider triggering manual GC",
                "Review large object allocations",
                "Implement memory pressure handling",
            },
        }
        mld.sendAlert(alert)
    }
}

// æ£€æµ‹å†…å­˜æ³„æ¼
func (mld *MemoryLeakDetector) detectMemoryLeak() {
    mld.historyMutex.RLock()
    defer mld.historyMutex.RUnlock()

    if len(mld.history) < 5 {
        return
    }

    // åˆ†ææœ€è¿‘5ä¸ªå¿«ç…§çš„è¶‹åŠ¿
    recent := mld.history[len(mld.history)-5:]

    // æ£€æŸ¥å†…å­˜æ˜¯å¦æŒç»­å¢é•¿
    isLeaking := true
    for i := 1; i < len(recent); i++ {
        if recent[i].HeapAlloc <= recent[i-1].HeapAlloc {
            isLeaking = false
            break
        }
    }

    if isLeaking {
        // è®¡ç®—å¹³å‡å¢é•¿ç‡
        totalGrowth := float64(recent[len(recent)-1].HeapAlloc - recent[0].HeapAlloc)
        avgGrowthRate := totalGrowth / float64(recent[0].HeapAlloc) * 100

        alert := &MemoryAlert{
            Type:      "memory_leak",
            Severity:  "critical",
            Message:   fmt.Sprintf("Potential memory leak detected: continuous growth over %v with average rate %.2f%%", mld.thresholds.LeakDetectionTime, avgGrowthRate),
            Timestamp: time.Now(),
            CurrentMem: recent[len(recent)-1].HeapAlloc,
            PreviousMem: recent[0].HeapAlloc,
            GrowthRate: avgGrowthRate,
            Suggestions: []string{
                "Perform heap dump analysis",
                "Review recent code changes for resource leaks",
                "Check for unclosed resources (files, connections, etc.)",
                "Analyze goroutine stack traces",
                "Consider implementing memory profiling",
            },
        }
        mld.sendAlert(alert)
    }
}

// å‘é€å‘Šè­¦
func (mld *MemoryLeakDetector) sendAlert(alert *MemoryAlert) {
    if mld.alertCallback != nil {
        mld.alertCallback(alert)
    }
}

// è·å–å†…å­˜ç»Ÿè®¡æŠ¥å‘Š
func (mld *MemoryLeakDetector) GetMemoryReport() *MemoryReport {
    mld.historyMutex.RLock()
    defer mld.historyMutex.RUnlock()

    if len(mld.history) == 0 {
        return nil
    }

    current := mld.history[len(mld.history)-1]

    report := &MemoryReport{
        Timestamp:      time.Now(),
        CurrentSnapshot: current,
        HistoryCount:   len(mld.history),
    }

    if len(mld.history) >= 2 {
        previous := mld.history[len(mld.history)-2]
        report.GrowthRate = float64(current.HeapAlloc-previous.HeapAlloc) / float64(previous.HeapAlloc) * 100
        report.GoroutineGrowth = current.GoroutineCount - previous.GoroutineCount
    }

    return report
}

// å†…å­˜æŠ¥å‘Š
type MemoryReport struct {
    Timestamp       time.Time      `json:"timestamp"`
    CurrentSnapshot MemorySnapshot `json:"current_snapshot"`
    HistoryCount    int            `json:"history_count"`
    GrowthRate      float64        `json:"growth_rate"`
    GoroutineGrowth int            `json:"goroutine_growth"`
    Recommendations []string       `json:"recommendations"`
}
```

### GCè°ƒä¼˜ç­–ç•¥

```go
// GCè°ƒä¼˜ç®¡ç†å™¨
package gc

import (
    "fmt"
    "runtime"
    "runtime/debug"
    "time"
)

// GCè°ƒä¼˜ç®¡ç†å™¨
type GCTuningManager struct {
    config          *GCConfig
    originalGCGoal  int
    originalMaxProcs int
    stats           *GCStats
}

// GCé…ç½®
type GCConfig struct {
    // GCç›®æ ‡ç™¾åˆ†æ¯” (é»˜è®¤100)
    GCGoal int `json:"gc_goal"`

    // æœ€å¤§å¹¶è¡ŒGCçº¿ç¨‹æ•°
    MaxGCProcs int `json:"max_gc_procs"`

    // å†…å­˜é™åˆ¶ (bytes)
    MemoryLimit int64 `json:"memory_limit"`

    // æ˜¯å¦å¯ç”¨GCè°ƒä¼˜
    Enabled bool `json:"enabled"`

    // è‡ªåŠ¨è°ƒä¼˜é…ç½®
    AutoTuning struct {
        Enabled           bool          `json:"enabled"`
        TargetLatency     time.Duration `json:"target_latency"`     // ç›®æ ‡GCå»¶è¿Ÿ
        TargetThroughput  float64       `json:"target_throughput"`  // ç›®æ ‡ååé‡
        AdjustmentFactor  float64       `json:"adjustment_factor"`  // è°ƒæ•´å› å­
        MonitorInterval   time.Duration `json:"monitor_interval"`   // ç›‘æ§é—´éš”
    } `json:"auto_tuning"`
}

// GCç»Ÿè®¡ä¿¡æ¯
type GCStats struct {
    NumGC           uint32        `json:"num_gc"`
    PauseTotal      time.Duration `json:"pause_total"`
    PauseAvg        time.Duration `json:"pause_avg"`
    PauseMax        time.Duration `json:"pause_max"`
    PauseMin        time.Duration `json:"pause_min"`
    LastGC          time.Time     `json:"last_gc"`
    GCCPUFraction   float64       `json:"gc_cpu_fraction"`
    HeapSize        uint64        `json:"heap_size"`
    NextGC          uint64        `json:"next_gc"`
    Frequency       float64       `json:"frequency"` // GCé¢‘ç‡ (æ¬¡/ç§’)
}

// åˆ›å»ºGCè°ƒä¼˜ç®¡ç†å™¨
func NewGCTuningManager(config *GCConfig) *GCTuningManager {
    return &GCTuningManager{
        config:          config,
        originalGCGoal:  debug.SetGCPercent(-1), // è·å–åŸå§‹å€¼
        originalMaxProcs: runtime.GOMAXPROCS(0),
        stats:           &GCStats{},
    }
}

// åº”ç”¨GCé…ç½®
func (gtm *GCTuningManager) ApplyGCConfig() {
    if !gtm.config.Enabled {
        return
    }

    // è®¾ç½®GCç›®æ ‡ç™¾åˆ†æ¯”
    if gtm.config.GCGoal > 0 {
        debug.SetGCPercent(gtm.config.GCGoal)
        fmt.Printf("Set GC goal to %d%%\n", gtm.config.GCGoal)
    }

    // è®¾ç½®å†…å­˜é™åˆ¶
    if gtm.config.MemoryLimit > 0 {
        debug.SetMemoryLimit(gtm.config.MemoryLimit)
        fmt.Printf("Set memory limit to %d bytes\n", gtm.config.MemoryLimit)
    }

    // è®¾ç½®æœ€å¤§GCçº¿ç¨‹æ•°
    if gtm.config.MaxGCProcs > 0 {
        runtime.GOMAXPROCS(gtm.config.MaxGCProcs)
        fmt.Printf("Set GOMAXPROCS to %d\n", gtm.config.MaxGCProcs)
    }
}

// æ¢å¤åŸå§‹GCé…ç½®
func (gtm *GCTuningManager) RestoreOriginalConfig() {
    debug.SetGCPercent(gtm.originalGCGoal)
    runtime.GOMAXPROCS(gtm.originalMaxProcs)
    debug.SetMemoryLimit(-1) // ç§»é™¤å†…å­˜é™åˆ¶
}

// æ”¶é›†GCç»Ÿè®¡ä¿¡æ¯
func (gtm *GCTuningManager) CollectGCStats() *GCStats {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)

    var gcStats debug.GCStats
    debug.ReadGCStats(&gcStats)

    // è®¡ç®—å¹³å‡æš‚åœæ—¶é—´
    var pauseAvg time.Duration
    if gcStats.NumGC > 0 {
        pauseAvg = gcStats.PauseTotal / time.Duration(gcStats.NumGC)
    }

    // è®¡ç®—æœ€å¤§å’Œæœ€å°æš‚åœæ—¶é—´
    var pauseMax, pauseMin time.Duration
    if len(gcStats.Pause) > 0 {
        pauseMax = gcStats.Pause[0]
        pauseMin = gcStats.Pause[0]
        for _, pause := range gcStats.Pause {
            if pause > pauseMax {
                pauseMax = pause
            }
            if pause < pauseMin && pause > 0 {
                pauseMin = pause
            }
        }
    }

    // è®¡ç®—GCé¢‘ç‡
    var frequency float64
    if !gcStats.LastGC.IsZero() {
        timeSinceLastGC := time.Since(gcStats.LastGC)
        if timeSinceLastGC > 0 {
            frequency = 1.0 / timeSinceLastGC.Seconds()
        }
    }

    gtm.stats = &GCStats{
        NumGC:         uint32(gcStats.NumGC),
        PauseTotal:    gcStats.PauseTotal,
        PauseAvg:      pauseAvg,
        PauseMax:      pauseMax,
        PauseMin:      pauseMin,
        LastGC:        gcStats.LastGC,
        GCCPUFraction: m.GCCPUFraction,
        HeapSize:      m.HeapAlloc,
        NextGC:        m.NextGC,
        Frequency:     frequency,
    }

    return gtm.stats
}

// è‡ªåŠ¨GCè°ƒä¼˜
func (gtm *GCTuningManager) AutoTuneGC() {
    if !gtm.config.AutoTuning.Enabled {
        return
    }

    stats := gtm.CollectGCStats()

    // æ ¹æ®ç›®æ ‡å»¶è¿Ÿè°ƒæ•´GCç›®æ ‡
    if stats.PauseAvg > gtm.config.AutoTuning.TargetLatency {
        // æš‚åœæ—¶é—´è¿‡é•¿ï¼Œé™ä½GCç›®æ ‡ç™¾åˆ†æ¯”
        newGoal := int(float64(gtm.config.GCGoal) * (1.0 - gtm.config.AutoTuning.AdjustmentFactor))
        if newGoal < 50 { // æœ€å°å€¼é™åˆ¶
            newGoal = 50
        }
        gtm.config.GCGoal = newGoal
        debug.SetGCPercent(newGoal)
        fmt.Printf("Auto-tuned GC goal to %d%% (reducing pause time)\n", newGoal)
    } else if stats.GCCPUFraction > gtm.config.AutoTuning.TargetThroughput {
        // GC CPUå ç”¨è¿‡é«˜ï¼Œå¢åŠ GCç›®æ ‡ç™¾åˆ†æ¯”
        newGoal := int(float64(gtm.config.GCGoal) * (1.0 + gtm.config.AutoTuning.AdjustmentFactor))
        if newGoal > 500 { // æœ€å¤§å€¼é™åˆ¶
            newGoal = 500
        }
        gtm.config.GCGoal = newGoal
        debug.SetGCPercent(newGoal)
        fmt.Printf("Auto-tuned GC goal to %d%% (improving throughput)\n", newGoal)
    }
}

// å¼ºåˆ¶GCå¹¶æµ‹é‡æ€§èƒ½
func (gtm *GCTuningManager) ForceGCAndMeasure() *GCMeasurement {
    start := time.Now()

    var beforeMem runtime.MemStats
    runtime.ReadMemStats(&beforeMem)

    // å¼ºåˆ¶æ‰§è¡ŒGC
    runtime.GC()

    var afterMem runtime.MemStats
    runtime.ReadMemStats(&afterMem)

    duration := time.Since(start)

    return &GCMeasurement{
        Duration:     duration,
        BeforeAlloc:  beforeMem.Alloc,
        AfterAlloc:   afterMem.Alloc,
        Freed:        beforeMem.Alloc - afterMem.Alloc,
        BeforeNextGC: beforeMem.NextGC,
        AfterNextGC:  afterMem.NextGC,
        Timestamp:    time.Now(),
    }
}

// GCæµ‹é‡ç»“æœ
type GCMeasurement struct {
    Duration     time.Duration `json:"duration"`
    BeforeAlloc  uint64        `json:"before_alloc"`
    AfterAlloc   uint64        `json:"after_alloc"`
    Freed        uint64        `json:"freed"`
    BeforeNextGC uint64        `json:"before_next_gc"`
    AfterNextGC  uint64        `json:"after_next_gc"`
    Timestamp    time.Time     `json:"timestamp"`
}

// è·å–GCè°ƒä¼˜å»ºè®®
func (gtm *GCTuningManager) GetTuningRecommendations() []string {
    stats := gtm.CollectGCStats()
    recommendations := make([]string, 0)

    // åŸºäºæš‚åœæ—¶é—´çš„å»ºè®®
    if stats.PauseAvg > 10*time.Millisecond {
        recommendations = append(recommendations,
            "GC pause time is high (>10ms). Consider reducing GOGC value or implementing object pooling.")
    }

    // åŸºäºGCé¢‘ç‡çš„å»ºè®®
    if stats.Frequency > 10 { // æ¯ç§’è¶…è¿‡10æ¬¡GC
        recommendations = append(recommendations,
            "GC frequency is high (>10/sec). Consider increasing GOGC value or reducing allocation rate.")
    }

    // åŸºäºCPUå ç”¨çš„å»ºè®®
    if stats.GCCPUFraction > 0.25 { // GCå ç”¨è¶…è¿‡25% CPU
        recommendations = append(recommendations,
            "GC CPU usage is high (>25%). Consider optimizing allocation patterns or increasing heap size.")
    }

    // åŸºäºå †å¤§å°çš„å»ºè®®
    if stats.HeapSize > 1<<30 { // å †å¤§å°è¶…è¿‡1GB
        recommendations = append(recommendations,
            "Heap size is large (>1GB). Consider implementing memory pressure handling and object lifecycle management.")
    }

    if len(recommendations) == 0 {
        recommendations = append(recommendations, "GC performance is within acceptable ranges.")
    }

    return recommendations
}
```

### å¯¹è±¡æ± ä¼˜åŒ–

å¯¹è±¡æ± æ˜¯å‡å°‘å†…å­˜åˆ†é…å’ŒGCå‹åŠ›çš„é‡è¦æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚ç”¨äºé¢‘ç¹åˆ›å»ºå’Œé”€æ¯çš„å¯¹è±¡ã€‚

```go
// å¯¹è±¡æ± ç®¡ç†å™¨
package pool

import (
    "bytes"
    "context"
    "fmt"
    "sync"
    "sync/atomic"
    "time"
)

// é€šç”¨å¯¹è±¡æ± æ¥å£
type ObjectPool interface {
    Get() interface{}
    Put(interface{})
    Size() int
    Stats() *PoolStats
    Clear()
}

// æ± ç»Ÿè®¡ä¿¡æ¯
type PoolStats struct {
    Gets        int64     `json:"gets"`         // è·å–æ¬¡æ•°
    Puts        int64     `json:"puts"`         // å½’è¿˜æ¬¡æ•°
    Hits        int64     `json:"hits"`         // å‘½ä¸­æ¬¡æ•°
    Misses      int64     `json:"misses"`       // æœªå‘½ä¸­æ¬¡æ•°
    Size        int       `json:"size"`         // å½“å‰å¤§å°
    MaxSize     int       `json:"max_size"`     // æœ€å¤§å¤§å°
    HitRate     float64   `json:"hit_rate"`     // å‘½ä¸­ç‡
    LastReset   time.Time `json:"last_reset"`   // ä¸Šæ¬¡é‡ç½®æ—¶é—´
}

// é«˜æ€§èƒ½å¯¹è±¡æ± 
type HighPerformancePool struct {
    pool        sync.Pool
    newFunc     func() interface{}
    resetFunc   func(interface{})
    maxSize     int
    currentSize int64
    stats       *PoolStats
    mutex       sync.RWMutex
}

// åˆ›å»ºé«˜æ€§èƒ½å¯¹è±¡æ± 
func NewHighPerformancePool(newFunc func() interface{}, resetFunc func(interface{}), maxSize int) *HighPerformancePool {
    return &HighPerformancePool{
        pool: sync.Pool{
            New: newFunc,
        },
        newFunc:   newFunc,
        resetFunc: resetFunc,
        maxSize:   maxSize,
        stats: &PoolStats{
            MaxSize:   maxSize,
            LastReset: time.Now(),
        },
    }
}

// è·å–å¯¹è±¡
func (hpp *HighPerformancePool) Get() interface{} {
    atomic.AddInt64(&hpp.stats.Gets, 1)

    obj := hpp.pool.Get()
    if obj != nil {
        atomic.AddInt64(&hpp.stats.Hits, 1)
        atomic.AddInt64(&hpp.currentSize, -1)
    } else {
        atomic.AddInt64(&hpp.stats.Misses, 1)
        obj = hpp.newFunc()
    }

    return obj
}

// å½’è¿˜å¯¹è±¡
func (hpp *HighPerformancePool) Put(obj interface{}) {
    if obj == nil {
        return
    }

    atomic.AddInt64(&hpp.stats.Puts, 1)

    // æ£€æŸ¥æ± å¤§å°é™åˆ¶
    if int(atomic.LoadInt64(&hpp.currentSize)) >= hpp.maxSize {
        return // ä¸¢å¼ƒå¯¹è±¡ï¼Œé¿å…æ± è¿‡å¤§
    }

    // é‡ç½®å¯¹è±¡çŠ¶æ€
    if hpp.resetFunc != nil {
        hpp.resetFunc(obj)
    }

    hpp.pool.Put(obj)
    atomic.AddInt64(&hpp.currentSize, 1)
}

// è·å–æ± å¤§å°
func (hpp *HighPerformancePool) Size() int {
    return int(atomic.LoadInt64(&hpp.currentSize))
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (hpp *HighPerformancePool) Stats() *PoolStats {
    hpp.mutex.RLock()
    defer hpp.mutex.RUnlock()

    gets := atomic.LoadInt64(&hpp.stats.Gets)
    hits := atomic.LoadInt64(&hpp.stats.Hits)

    stats := *hpp.stats
    stats.Gets = gets
    stats.Puts = atomic.LoadInt64(&hpp.stats.Puts)
    stats.Hits = hits
    stats.Misses = atomic.LoadInt64(&hpp.stats.Misses)
    stats.Size = hpp.Size()

    if gets > 0 {
        stats.HitRate = float64(hits) / float64(gets) * 100
    }

    return &stats
}

// æ¸…ç©ºæ± 
func (hpp *HighPerformancePool) Clear() {
    // åˆ›å»ºæ–°çš„sync.Poolæ¥æ¸…ç©º
    hpp.pool = sync.Pool{
        New: hpp.newFunc,
    }
    atomic.StoreInt64(&hpp.currentSize, 0)
}

// å­—èŠ‚ç¼“å†²æ± 
type ByteBufferPool struct {
    pool        *HighPerformancePool
    initialSize int
    maxSize     int
}

// åˆ›å»ºå­—èŠ‚ç¼“å†²æ± 
func NewByteBufferPool(initialSize, maxSize int, poolMaxSize int) *ByteBufferPool {
    bbp := &ByteBufferPool{
        initialSize: initialSize,
        maxSize:     maxSize,
    }

    bbp.pool = NewHighPerformancePool(
        func() interface{} {
            return bytes.NewBuffer(make([]byte, 0, bbp.initialSize))
        },
        func(obj interface{}) {
            if buf, ok := obj.(*bytes.Buffer); ok {
                buf.Reset()
                // å¦‚æœç¼“å†²åŒºå¤ªå¤§ï¼Œé‡æ–°åˆ›å»º
                if buf.Cap() > bbp.maxSize {
                    buf = bytes.NewBuffer(make([]byte, 0, bbp.initialSize))
                }
            }
        },
        poolMaxSize,
    )

    return bbp
}

// è·å–å­—èŠ‚ç¼“å†²
func (bbp *ByteBufferPool) Get() *bytes.Buffer {
    return bbp.pool.Get().(*bytes.Buffer)
}

// å½’è¿˜å­—èŠ‚ç¼“å†²
func (bbp *ByteBufferPool) Put(buf *bytes.Buffer) {
    bbp.pool.Put(buf)
}

// HTTPå“åº”æ± 
type HTTPResponsePool struct {
    pool *HighPerformancePool
}

// HTTPå“åº”å¯¹è±¡
type HTTPResponse struct {
    StatusCode int               `json:"status_code"`
    Headers    map[string]string `json:"headers"`
    Body       []byte            `json:"body"`
    Timestamp  time.Time         `json:"timestamp"`
}

// åˆ›å»ºHTTPå“åº”æ± 
func NewHTTPResponsePool(maxSize int) *HTTPResponsePool {
    hrp := &HTTPResponsePool{}

    hrp.pool = NewHighPerformancePool(
        func() interface{} {
            return &HTTPResponse{
                Headers: make(map[string]string),
                Body:    make([]byte, 0, 1024),
            }
        },
        func(obj interface{}) {
            if resp, ok := obj.(*HTTPResponse); ok {
                resp.StatusCode = 0
                // æ¸…ç©ºheadersä½†ä¿ç•™å®¹é‡
                for k := range resp.Headers {
                    delete(resp.Headers, k)
                }
                resp.Body = resp.Body[:0] // ä¿ç•™å®¹é‡
                resp.Timestamp = time.Time{}
            }
        },
        maxSize,
    )

    return hrp
}

// è·å–HTTPå“åº”å¯¹è±¡
func (hrp *HTTPResponsePool) Get() *HTTPResponse {
    return hrp.pool.Get().(*HTTPResponse)
}

// å½’è¿˜HTTPå“åº”å¯¹è±¡
func (hrp *HTTPResponsePool) Put(resp *HTTPResponse) {
    hrp.pool.Put(resp)
}

// è¿æ¥æ± ç®¡ç†å™¨
type ConnectionPool struct {
    pool        chan interface{}
    factory     func() (interface{}, error)
    close       func(interface{}) error
    ping        func(interface{}) error
    maxIdle     int
    maxActive   int
    idleTimeout time.Duration
    active      int64
    stats       *PoolStats
    mutex       sync.RWMutex
}

// è¿æ¥æ± é…ç½®
type ConnectionPoolConfig struct {
    MaxIdle     int           `json:"max_idle"`
    MaxActive   int           `json:"max_active"`
    IdleTimeout time.Duration `json:"idle_timeout"`
    Factory     func() (interface{}, error)
    Close       func(interface{}) error
    Ping        func(interface{}) error
}

// åˆ›å»ºè¿æ¥æ± 
func NewConnectionPool(config *ConnectionPoolConfig) *ConnectionPool {
    return &ConnectionPool{
        pool:        make(chan interface{}, config.MaxIdle),
        factory:     config.Factory,
        close:       config.Close,
        ping:        config.Ping,
        maxIdle:     config.MaxIdle,
        maxActive:   config.MaxActive,
        idleTimeout: config.IdleTimeout,
        stats: &PoolStats{
            MaxSize:   config.MaxIdle,
            LastReset: time.Now(),
        },
    }
}

// è·å–è¿æ¥
func (cp *ConnectionPool) Get() (interface{}, error) {
    atomic.AddInt64(&cp.stats.Gets, 1)

    // å°è¯•ä»æ± ä¸­è·å–è¿æ¥
    select {
    case conn := <-cp.pool:
        // æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
        if cp.ping != nil && cp.ping(conn) != nil {
            cp.close(conn)
            atomic.AddInt64(&cp.stats.Misses, 1)
            return cp.createConnection()
        }
        atomic.AddInt64(&cp.stats.Hits, 1)
        return conn, nil
    default:
        // æ± ä¸­æ²¡æœ‰å¯ç”¨è¿æ¥ï¼Œåˆ›å»ºæ–°è¿æ¥
        atomic.AddInt64(&cp.stats.Misses, 1)
        return cp.createConnection()
    }
}

// åˆ›å»ºæ–°è¿æ¥
func (cp *ConnectionPool) createConnection() (interface{}, error) {
    // æ£€æŸ¥æ´»è·ƒè¿æ¥æ•°é™åˆ¶
    if int(atomic.LoadInt64(&cp.active)) >= cp.maxActive {
        return nil, fmt.Errorf("connection pool exhausted")
    }

    conn, err := cp.factory()
    if err != nil {
        return nil, err
    }

    atomic.AddInt64(&cp.active, 1)
    return conn, nil
}

// å½’è¿˜è¿æ¥
func (cp *ConnectionPool) Put(conn interface{}) {
    if conn == nil {
        return
    }

    atomic.AddInt64(&cp.stats.Puts, 1)

    // å°è¯•å°†è¿æ¥æ”¾å›æ± ä¸­
    select {
    case cp.pool <- conn:
        // æˆåŠŸæ”¾å›æ± ä¸­
    default:
        // æ± å·²æ»¡ï¼Œå…³é—­è¿æ¥
        cp.close(conn)
        atomic.AddInt64(&cp.active, -1)
    }
}

// å…³é—­è¿æ¥æ± 
func (cp *ConnectionPool) Close() error {
    close(cp.pool)

    // å…³é—­æ‰€æœ‰æ± ä¸­çš„è¿æ¥
    for conn := range cp.pool {
        cp.close(conn)
        atomic.AddInt64(&cp.active, -1)
    }

    return nil
}

// æ± ç®¡ç†å™¨
type PoolManager struct {
    pools map[string]ObjectPool
    mutex sync.RWMutex
}

// åˆ›å»ºæ± ç®¡ç†å™¨
func NewPoolManager() *PoolManager {
    return &PoolManager{
        pools: make(map[string]ObjectPool),
    }
}

// æ³¨å†Œæ± 
func (pm *PoolManager) RegisterPool(name string, pool ObjectPool) {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    pm.pools[name] = pool
}

// è·å–æ± 
func (pm *PoolManager) GetPool(name string) ObjectPool {
    pm.mutex.RLock()
    defer pm.mutex.RUnlock()
    return pm.pools[name]
}

// è·å–æ‰€æœ‰æ± çš„ç»Ÿè®¡ä¿¡æ¯
func (pm *PoolManager) GetAllStats() map[string]*PoolStats {
    pm.mutex.RLock()
    defer pm.mutex.RUnlock()

    stats := make(map[string]*PoolStats)
    for name, pool := range pm.pools {
        stats[name] = pool.Stats()
    }
    return stats
}

// æ¸…ç©ºæ‰€æœ‰æ± 
func (pm *PoolManager) ClearAll() {
    pm.mutex.RLock()
    defer pm.mutex.RUnlock()

    for _, pool := range pm.pools {
        pool.Clear()
    }
}
```

---

## âš¡ å¹¶å‘æ€§èƒ½è°ƒä¼˜

### Goroutineæ± ä¼˜åŒ–

```go
// Goroutineæ± ç®¡ç†å™¨
package goroutine

import (
    "context"
    "fmt"
    "runtime"
    "sync"
    "sync/atomic"
    "time"
)

// å·¥ä½œä»»åŠ¡æ¥å£
type Task interface {
    Execute(ctx context.Context) error
}

// å‡½æ•°ä»»åŠ¡
type FuncTask struct {
    Fn func(ctx context.Context) error
}

func (ft *FuncTask) Execute(ctx context.Context) error {
    return ft.Fn(ctx)
}

// Goroutineæ± 
type GoroutinePool struct {
    // é…ç½®å‚æ•°
    minWorkers    int
    maxWorkers    int
    maxIdleTime   time.Duration
    taskQueueSize int

    // è¿è¡Œæ—¶çŠ¶æ€
    workers       int64
    activeWorkers int64
    taskQueue     chan Task
    workerQueue   chan chan Task
    quit          chan bool
    wg            sync.WaitGroup

    // ç»Ÿè®¡ä¿¡æ¯
    stats         *PoolStats

    // æ§åˆ¶
    mutex         sync.RWMutex
    started       bool
}

// æ± ç»Ÿè®¡ä¿¡æ¯
type PoolStats struct {
    Workers       int64     `json:"workers"`
    ActiveWorkers int64     `json:"active_workers"`
    QueuedTasks   int       `json:"queued_tasks"`
    CompletedTasks int64    `json:"completed_tasks"`
    FailedTasks   int64     `json:"failed_tasks"`
    TotalTasks    int64     `json:"total_tasks"`
    AvgTaskTime   time.Duration `json:"avg_task_time"`
    LastTaskTime  time.Time `json:"last_task_time"`
}

// æ± é…ç½®
type PoolConfig struct {
    MinWorkers    int           `json:"min_workers"`
    MaxWorkers    int           `json:"max_workers"`
    MaxIdleTime   time.Duration `json:"max_idle_time"`
    TaskQueueSize int           `json:"task_queue_size"`
}

// åˆ›å»ºGoroutineæ± 
func NewGoroutinePool(config *PoolConfig) *GoroutinePool {
    if config.MinWorkers <= 0 {
        config.MinWorkers = 1
    }
    if config.MaxWorkers <= 0 {
        config.MaxWorkers = runtime.NumCPU() * 2
    }
    if config.MaxIdleTime <= 0 {
        config.MaxIdleTime = 30 * time.Second
    }
    if config.TaskQueueSize <= 0 {
        config.TaskQueueSize = 1000
    }

    return &GoroutinePool{
        minWorkers:    config.MinWorkers,
        maxWorkers:    config.MaxWorkers,
        maxIdleTime:   config.MaxIdleTime,
        taskQueueSize: config.TaskQueueSize,
        taskQueue:     make(chan Task, config.TaskQueueSize),
        workerQueue:   make(chan chan Task, config.MaxWorkers),
        quit:          make(chan bool),
        stats:         &PoolStats{},
    }
}

// å¯åŠ¨æ± 
func (gp *GoroutinePool) Start() error {
    gp.mutex.Lock()
    defer gp.mutex.Unlock()

    if gp.started {
        return fmt.Errorf("pool already started")
    }

    // å¯åŠ¨æœ€å°æ•°é‡çš„worker
    for i := 0; i < gp.minWorkers; i++ {
        gp.startWorker()
    }

    // å¯åŠ¨è°ƒåº¦å™¨
    go gp.dispatcher()

    // å¯åŠ¨ç›‘æ§å™¨
    go gp.monitor()

    gp.started = true
    return nil
}

// åœæ­¢æ± 
func (gp *GoroutinePool) Stop() error {
    gp.mutex.Lock()
    defer gp.mutex.Unlock()

    if !gp.started {
        return fmt.Errorf("pool not started")
    }

    close(gp.quit)
    gp.wg.Wait()

    gp.started = false
    return nil
}

// æäº¤ä»»åŠ¡
func (gp *GoroutinePool) Submit(task Task) error {
    if !gp.started {
        return fmt.Errorf("pool not started")
    }

    select {
    case gp.taskQueue <- task:
        atomic.AddInt64(&gp.stats.TotalTasks, 1)
        return nil
    default:
        return fmt.Errorf("task queue full")
    }
}

// æäº¤å‡½æ•°ä»»åŠ¡
func (gp *GoroutinePool) SubmitFunc(fn func(ctx context.Context) error) error {
    return gp.Submit(&FuncTask{Fn: fn})
}

// è°ƒåº¦å™¨
func (gp *GoroutinePool) dispatcher() {
    for {
        select {
        case task := <-gp.taskQueue:
            // å°è¯•è·å–å¯ç”¨worker
            select {
            case workerTaskQueue := <-gp.workerQueue:
                // æœ‰å¯ç”¨workerï¼Œåˆ†é…ä»»åŠ¡
                workerTaskQueue <- task
            default:
                // æ²¡æœ‰å¯ç”¨workerï¼Œå°è¯•åˆ›å»ºæ–°worker
                if atomic.LoadInt64(&gp.workers) < int64(gp.maxWorkers) {
                    gp.startWorker()
                    // é‡æ–°å°è¯•åˆ†é…ä»»åŠ¡
                    select {
                    case workerTaskQueue := <-gp.workerQueue:
                        workerTaskQueue <- task
                    case <-time.After(100 * time.Millisecond):
                        // è¶…æ—¶ï¼Œå°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—
                        select {
                        case gp.taskQueue <- task:
                        default:
                            // é˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒä»»åŠ¡
                            atomic.AddInt64(&gp.stats.FailedTasks, 1)
                        }
                    }
                } else {
                    // å·²è¾¾åˆ°æœ€å¤§workeræ•°ï¼Œç­‰å¾…å¯ç”¨worker
                    select {
                    case workerTaskQueue := <-gp.workerQueue:
                        workerTaskQueue <- task
                    case <-time.After(1 * time.Second):
                        // è¶…æ—¶ï¼Œå°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—
                        select {
                        case gp.taskQueue <- task:
                        default:
                            atomic.AddInt64(&gp.stats.FailedTasks, 1)
                        }
                    }
                }
            }
        case <-gp.quit:
            return
        }
    }
}

// å¯åŠ¨worker
func (gp *GoroutinePool) startWorker() {
    atomic.AddInt64(&gp.workers, 1)
    gp.wg.Add(1)

    go func() {
        defer func() {
            atomic.AddInt64(&gp.workers, -1)
            gp.wg.Done()
        }()

        taskQueue := make(chan Task)
        idleTimer := time.NewTimer(gp.maxIdleTime)

        for {
            // å°†workeræ³¨å†Œåˆ°workeré˜Ÿåˆ—
            select {
            case gp.workerQueue <- taskQueue:
            case <-gp.quit:
                return
            }

            // ç­‰å¾…ä»»åŠ¡æˆ–è¶…æ—¶
            select {
            case task := <-taskQueue:
                // é‡ç½®ç©ºé—²è®¡æ—¶å™¨
                if !idleTimer.Stop() {
                    <-idleTimer.C
                }
                idleTimer.Reset(gp.maxIdleTime)

                // æ‰§è¡Œä»»åŠ¡
                gp.executeTask(task)

            case <-idleTimer.C:
                // ç©ºé—²è¶…æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é€€å‡º
                if atomic.LoadInt64(&gp.workers) > int64(gp.minWorkers) {
                    return
                }
                idleTimer.Reset(gp.maxIdleTime)

            case <-gp.quit:
                return
            }
        }
    }()
}

// æ‰§è¡Œä»»åŠ¡
func (gp *GoroutinePool) executeTask(task Task) {
    start := time.Now()
    atomic.AddInt64(&gp.activeWorkers, 1)

    defer func() {
        atomic.AddInt64(&gp.activeWorkers, -1)
        duration := time.Since(start)

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        atomic.AddInt64(&gp.stats.CompletedTasks, 1)
        gp.stats.LastTaskTime = time.Now()

        // è®¡ç®—å¹³å‡ä»»åŠ¡æ—¶é—´
        completed := atomic.LoadInt64(&gp.stats.CompletedTasks)
        if completed > 0 {
            gp.stats.AvgTaskTime = time.Duration(int64(gp.stats.AvgTaskTime)*completed + int64(duration)) / time.Duration(completed+1)
        }

        // æ¢å¤panic
        if r := recover(); r != nil {
            atomic.AddInt64(&gp.stats.FailedTasks, 1)
            fmt.Printf("Task panic recovered: %v\n", r)
        }
    }()

    // æ‰§è¡Œä»»åŠ¡
    ctx := context.Background()
    if err := task.Execute(ctx); err != nil {
        atomic.AddInt64(&gp.stats.FailedTasks, 1)
        fmt.Printf("Task execution failed: %v\n", err)
    }
}

// ç›‘æ§å™¨
func (gp *GoroutinePool) monitor() {
    ticker := time.NewTicker(10 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            gp.updateStats()
            gp.adjustWorkers()
        case <-gp.quit:
            return
        }
    }
}

// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
func (gp *GoroutinePool) updateStats() {
    gp.stats.Workers = atomic.LoadInt64(&gp.workers)
    gp.stats.ActiveWorkers = atomic.LoadInt64(&gp.activeWorkers)
    gp.stats.QueuedTasks = len(gp.taskQueue)
}

// åŠ¨æ€è°ƒæ•´workeræ•°é‡
func (gp *GoroutinePool) adjustWorkers() {
    queuedTasks := len(gp.taskQueue)
    workers := atomic.LoadInt64(&gp.workers)
    activeWorkers := atomic.LoadInt64(&gp.activeWorkers)

    // å¦‚æœé˜Ÿåˆ—ä¸­æœ‰å¾ˆå¤šä»»åŠ¡ä¸”æ´»è·ƒworkeræ¯”ä¾‹é«˜ï¼Œå¢åŠ worker
    if queuedTasks > gp.taskQueueSize/2 && float64(activeWorkers)/float64(workers) > 0.8 {
        if workers < int64(gp.maxWorkers) {
            gp.startWorker()
            fmt.Printf("Increased workers to %d due to high load\n", workers+1)
        }
    }

    // å¦‚æœé˜Ÿåˆ—ç©ºä¸”æ´»è·ƒworkeræ¯”ä¾‹ä½ï¼Œå¯èƒ½éœ€è¦å‡å°‘workerï¼ˆç”±ç©ºé—²è¶…æ—¶å¤„ç†ï¼‰
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (gp *GoroutinePool) Stats() *PoolStats {
    gp.updateStats()
    return gp.stats
}

// è·å–æ± çŠ¶æ€
func (gp *GoroutinePool) Status() map[string]interface{} {
    stats := gp.Stats()
    return map[string]interface{}{
        "workers":         stats.Workers,
        "active_workers":  stats.ActiveWorkers,
        "queued_tasks":    stats.QueuedTasks,
        "completed_tasks": stats.CompletedTasks,
        "failed_tasks":    stats.FailedTasks,
        "total_tasks":     stats.TotalTasks,
        "avg_task_time":   stats.AvgTaskTime,
        "last_task_time":  stats.LastTaskTime,
        "utilization":     float64(stats.ActiveWorkers) / float64(stats.Workers) * 100,
        "queue_usage":     float64(stats.QueuedTasks) / float64(gp.taskQueueSize) * 100,
    }
}
```

### Channelä¼˜åŒ–ç­–ç•¥

```go
// Channelä¼˜åŒ–ç®¡ç†å™¨
package channel

import (
    "context"
    "fmt"
    "sync"
    "sync/atomic"
    "time"
)

// é«˜æ€§èƒ½ChannelåŒ…è£…å™¨
type OptimizedChannel struct {
    ch          chan interface{}
    capacity    int
    sendCount   int64
    recvCount   int64
    blockCount  int64
    closeOnce   sync.Once
    closed      int32
    stats       *ChannelStats
}

// Channelç»Ÿè®¡ä¿¡æ¯
type ChannelStats struct {
    Capacity      int           `json:"capacity"`
    Length        int           `json:"length"`
    SendCount     int64         `json:"send_count"`
    RecvCount     int64         `json:"recv_count"`
    BlockCount    int64         `json:"block_count"`
    Throughput    float64       `json:"throughput"`    // æ¶ˆæ¯/ç§’
    AvgLatency    time.Duration `json:"avg_latency"`
    LastActivity  time.Time     `json:"last_activity"`
}

// åˆ›å»ºä¼˜åŒ–çš„Channel
func NewOptimizedChannel(capacity int) *OptimizedChannel {
    return &OptimizedChannel{
        ch:       make(chan interface{}, capacity),
        capacity: capacity,
        stats:    &ChannelStats{Capacity: capacity},
    }
}

// éé˜»å¡å‘é€
func (oc *OptimizedChannel) TrySend(data interface{}) bool {
    if atomic.LoadInt32(&oc.closed) == 1 {
        return false
    }

    select {
    case oc.ch <- data:
        atomic.AddInt64(&oc.sendCount, 1)
        oc.stats.LastActivity = time.Now()
        return true
    default:
        atomic.AddInt64(&oc.blockCount, 1)
        return false
    }
}

// å¸¦è¶…æ—¶çš„å‘é€
func (oc *OptimizedChannel) SendWithTimeout(data interface{}, timeout time.Duration) error {
    if atomic.LoadInt32(&oc.closed) == 1 {
        return fmt.Errorf("channel closed")
    }

    select {
    case oc.ch <- data:
        atomic.AddInt64(&oc.sendCount, 1)
        oc.stats.LastActivity = time.Now()
        return nil
    case <-time.After(timeout):
        atomic.AddInt64(&oc.blockCount, 1)
        return fmt.Errorf("send timeout")
    }
}

// éé˜»å¡æ¥æ”¶
func (oc *OptimizedChannel) TryRecv() (interface{}, bool) {
    select {
    case data := <-oc.ch:
        atomic.AddInt64(&oc.recvCount, 1)
        oc.stats.LastActivity = time.Now()
        return data, true
    default:
        return nil, false
    }
}

// å¸¦è¶…æ—¶çš„æ¥æ”¶
func (oc *OptimizedChannel) RecvWithTimeout(timeout time.Duration) (interface{}, error) {
    select {
    case data := <-oc.ch:
        atomic.AddInt64(&oc.recvCount, 1)
        oc.stats.LastActivity = time.Now()
        return data, nil
    case <-time.After(timeout):
        return nil, fmt.Errorf("recv timeout")
    }
}

// å…³é—­Channel
func (oc *OptimizedChannel) Close() {
    oc.closeOnce.Do(func() {
        atomic.StoreInt32(&oc.closed, 1)
        close(oc.ch)
    })
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (oc *OptimizedChannel) Stats() *ChannelStats {
    stats := *oc.stats
    stats.Length = len(oc.ch)
    stats.SendCount = atomic.LoadInt64(&oc.sendCount)
    stats.RecvCount = atomic.LoadInt64(&oc.recvCount)
    stats.BlockCount = atomic.LoadInt64(&oc.blockCount)

    // è®¡ç®—ååé‡
    if !stats.LastActivity.IsZero() {
        duration := time.Since(stats.LastActivity)
        if duration > 0 {
            stats.Throughput = float64(stats.SendCount+stats.RecvCount) / duration.Seconds()
        }
    }

    return &stats
}

// æ‰¹é‡Channelå¤„ç†å™¨
type BatchChannelProcessor struct {
    input       chan interface{}
    output      chan []interface{}
    batchSize   int
    flushTime   time.Duration
    processor   func([]interface{}) []interface{}
    ctx         context.Context
    cancel      context.CancelFunc
    wg          sync.WaitGroup
}

// åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
func NewBatchChannelProcessor(batchSize int, flushTime time.Duration, processor func([]interface{}) []interface{}) *BatchChannelProcessor {
    ctx, cancel := context.WithCancel(context.Background())

    return &BatchChannelProcessor{
        input:     make(chan interface{}, batchSize*2),
        output:    make(chan []interface{}, 10),
        batchSize: batchSize,
        flushTime: flushTime,
        processor: processor,
        ctx:       ctx,
        cancel:    cancel,
    }
}

// å¯åŠ¨æ‰¹é‡å¤„ç†
func (bcp *BatchChannelProcessor) Start() {
    bcp.wg.Add(1)
    go bcp.processBatches()
}

// åœæ­¢æ‰¹é‡å¤„ç†
func (bcp *BatchChannelProcessor) Stop() {
    bcp.cancel()
    bcp.wg.Wait()
    close(bcp.input)
    close(bcp.output)
}

// å‘é€æ•°æ®
func (bcp *BatchChannelProcessor) Send(data interface{}) error {
    select {
    case bcp.input <- data:
        return nil
    case <-bcp.ctx.Done():
        return bcp.ctx.Err()
    }
}

// æ¥æ”¶å¤„ç†ç»“æœ
func (bcp *BatchChannelProcessor) Recv() ([]interface{}, error) {
    select {
    case result := <-bcp.output:
        return result, nil
    case <-bcp.ctx.Done():
        return nil, bcp.ctx.Err()
    }
}

// æ‰¹é‡å¤„ç†é€»è¾‘
func (bcp *BatchChannelProcessor) processBatches() {
    defer bcp.wg.Done()

    batch := make([]interface{}, 0, bcp.batchSize)
    flushTimer := time.NewTimer(bcp.flushTime)

    for {
        select {
        case data := <-bcp.input:
            batch = append(batch, data)

            // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡å¤§å°
            if len(batch) >= bcp.batchSize {
                bcp.processBatch(batch)
                batch = batch[:0] // é‡ç½®åˆ‡ç‰‡

                // é‡ç½®å®šæ—¶å™¨
                if !flushTimer.Stop() {
                    <-flushTimer.C
                }
                flushTimer.Reset(bcp.flushTime)
            }

        case <-flushTimer.C:
            // å®šæ—¶åˆ·æ–°
            if len(batch) > 0 {
                bcp.processBatch(batch)
                batch = batch[:0]
            }
            flushTimer.Reset(bcp.flushTime)

        case <-bcp.ctx.Done():
            // å¤„ç†å‰©ä½™æ•°æ®
            if len(batch) > 0 {
                bcp.processBatch(batch)
            }
            return
        }
    }
}

// å¤„ç†å•ä¸ªæ‰¹æ¬¡
func (bcp *BatchChannelProcessor) processBatch(batch []interface{}) {
    if bcp.processor != nil {
        result := bcp.processor(batch)
        if result != nil {
            select {
            case bcp.output <- result:
            case <-bcp.ctx.Done():
                return
            }
        }
    }
}
```

### é”ä¼˜åŒ–ç­–ç•¥

```go
// é”ä¼˜åŒ–ç®¡ç†å™¨
package locks

import (
    "runtime"
    "sync"
    "sync/atomic"
    "time"
    "unsafe"
)

// è¯»å†™é”ç»Ÿè®¡ä¿¡æ¯
type RWMutexStats struct {
    ReadLocks    int64         `json:"read_locks"`
    WriteLocks   int64         `json:"write_locks"`
    ReadWaits    int64         `json:"read_waits"`
    WriteWaits   int64         `json:"write_waits"`
    AvgReadTime  time.Duration `json:"avg_read_time"`
    AvgWriteTime time.Duration `json:"avg_write_time"`
    Contention   float64       `json:"contention"`
}

// ä¼˜åŒ–çš„è¯»å†™é”
type OptimizedRWMutex struct {
    mu           sync.RWMutex
    stats        *RWMutexStats
    readCount    int64
    writeCount   int64
    readWaits    int64
    writeWaits   int64
    readTime     int64  // çº³ç§’
    writeTime    int64  // çº³ç§’
}

// åˆ›å»ºä¼˜åŒ–çš„è¯»å†™é”
func NewOptimizedRWMutex() *OptimizedRWMutex {
    return &OptimizedRWMutex{
        stats: &RWMutexStats{},
    }
}

// è¯»é”
func (orm *OptimizedRWMutex) RLock() {
    start := time.Now()

    // å°è¯•å¿«é€Ÿè·å–è¯»é”
    if orm.tryRLock() {
        atomic.AddInt64(&orm.readCount, 1)
        return
    }

    // å¿«é€Ÿè·å–å¤±è´¥ï¼Œè®°å½•ç­‰å¾…
    atomic.AddInt64(&orm.readWaits, 1)
    orm.mu.RLock()

    duration := time.Since(start)
    atomic.AddInt64(&orm.readCount, 1)
    atomic.AddInt64(&orm.readTime, int64(duration))
}

// è¯»è§£é”
func (orm *OptimizedRWMutex) RUnlock() {
    orm.mu.RUnlock()
}

// å†™é”
func (orm *OptimizedRWMutex) Lock() {
    start := time.Now()

    // å°è¯•å¿«é€Ÿè·å–å†™é”
    if orm.tryLock() {
        atomic.AddInt64(&orm.writeCount, 1)
        return
    }

    // å¿«é€Ÿè·å–å¤±è´¥ï¼Œè®°å½•ç­‰å¾…
    atomic.AddInt64(&orm.writeWaits, 1)
    orm.mu.Lock()

    duration := time.Since(start)
    atomic.AddInt64(&orm.writeCount, 1)
    atomic.AddInt64(&orm.writeTime, int64(duration))
}

// å†™è§£é”
func (orm *OptimizedRWMutex) Unlock() {
    orm.mu.Unlock()
}

// å°è¯•è·å–è¯»é”ï¼ˆéé˜»å¡ï¼‰
func (orm *OptimizedRWMutex) tryRLock() bool {
    // ä½¿ç”¨unsafeæ“ä½œå°è¯•å¿«é€Ÿè·¯å¾„
    // æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…æƒ…å†µæ›´å¤æ‚
    return false // ç®€åŒ–å®ç°ï¼Œæ€»æ˜¯è¿”å›false
}

// å°è¯•è·å–å†™é”ï¼ˆéé˜»å¡ï¼‰
func (orm *OptimizedRWMutex) tryLock() bool {
    // ä½¿ç”¨unsafeæ“ä½œå°è¯•å¿«é€Ÿè·¯å¾„
    return false // ç®€åŒ–å®ç°ï¼Œæ€»æ˜¯è¿”å›false
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (orm *OptimizedRWMutex) Stats() *RWMutexStats {
    readCount := atomic.LoadInt64(&orm.readCount)
    writeCount := atomic.LoadInt64(&orm.writeCount)
    readWaits := atomic.LoadInt64(&orm.readWaits)
    writeWaits := atomic.LoadInt64(&orm.writeWaits)
    readTime := atomic.LoadInt64(&orm.readTime)
    writeTime := atomic.LoadInt64(&orm.writeTime)

    stats := &RWMutexStats{
        ReadLocks:  readCount,
        WriteLocks: writeCount,
        ReadWaits:  readWaits,
        WriteWaits: writeWaits,
    }

    // è®¡ç®—å¹³å‡æ—¶é—´
    if readCount > 0 {
        stats.AvgReadTime = time.Duration(readTime / readCount)
    }
    if writeCount > 0 {
        stats.AvgWriteTime = time.Duration(writeTime / writeCount)
    }

    // è®¡ç®—ç«äº‰ç‡
    totalOps := readCount + writeCount
    totalWaits := readWaits + writeWaits
    if totalOps > 0 {
        stats.Contention = float64(totalWaits) / float64(totalOps) * 100
    }

    return stats
}

// è‡ªé€‚åº”è‡ªæ—‹é”
type AdaptiveSpinLock struct {
    state       int32
    spinCount   int32
    maxSpin     int32
    backoff     time.Duration
}

// åˆ›å»ºè‡ªé€‚åº”è‡ªæ—‹é”
func NewAdaptiveSpinLock() *AdaptiveSpinLock {
    return &AdaptiveSpinLock{
        maxSpin: int32(runtime.NumCPU() * 4),
        backoff: time.Microsecond,
    }
}

// è·å–é”
func (asl *AdaptiveSpinLock) Lock() {
    for {
        // å°è¯•åŸå­è·å–é”
        if atomic.CompareAndSwapInt32(&asl.state, 0, 1) {
            return
        }

        // è‡ªé€‚åº”è‡ªæ—‹
        spinCount := atomic.LoadInt32(&asl.spinCount)
        if spinCount < asl.maxSpin {
            // è‡ªæ—‹ç­‰å¾…
            for i := int32(0); i < spinCount; i++ {
                if atomic.LoadInt32(&asl.state) == 0 {
                    break
                }
                runtime.Gosched() // è®©å‡ºCPU
            }

            // å¢åŠ è‡ªæ—‹æ¬¡æ•°
            atomic.AddInt32(&asl.spinCount, 1)
        } else {
            // è‡ªæ—‹æ¬¡æ•°è¿‡å¤šï¼Œä½¿ç”¨é€€é¿ç­–ç•¥
            time.Sleep(asl.backoff)
            asl.backoff *= 2
            if asl.backoff > time.Millisecond {
                asl.backoff = time.Millisecond
            }

            // é‡ç½®è‡ªæ—‹è®¡æ•°
            atomic.StoreInt32(&asl.spinCount, 0)
        }
    }
}

// é‡Šæ”¾é”
func (asl *AdaptiveSpinLock) Unlock() {
    atomic.StoreInt32(&asl.state, 0)

    // é‡ç½®é€€é¿æ—¶é—´
    asl.backoff = time.Microsecond
}

// æ— é”é˜Ÿåˆ—
type LockFreeQueue struct {
    head unsafe.Pointer
    tail unsafe.Pointer
}

type queueNode struct {
    data interface{}
    next unsafe.Pointer
}

// åˆ›å»ºæ— é”é˜Ÿåˆ—
func NewLockFreeQueue() *LockFreeQueue {
    node := &queueNode{}
    return &LockFreeQueue{
        head: unsafe.Pointer(node),
        tail: unsafe.Pointer(node),
    }
}

// å…¥é˜Ÿ
func (lfq *LockFreeQueue) Enqueue(data interface{}) {
    node := &queueNode{data: data}

    for {
        tail := (*queueNode)(atomic.LoadPointer(&lfq.tail))
        next := (*queueNode)(atomic.LoadPointer(&tail.next))

        if tail == (*queueNode)(atomic.LoadPointer(&lfq.tail)) {
            if next == nil {
                if atomic.CompareAndSwapPointer(&tail.next, unsafe.Pointer(next), unsafe.Pointer(node)) {
                    break
                }
            } else {
                atomic.CompareAndSwapPointer(&lfq.tail, unsafe.Pointer(tail), unsafe.Pointer(next))
            }
        }
    }

    atomic.CompareAndSwapPointer(&lfq.tail, unsafe.Pointer((*queueNode)(atomic.LoadPointer(&lfq.tail))), unsafe.Pointer(node))
}

// å‡ºé˜Ÿ
func (lfq *LockFreeQueue) Dequeue() (interface{}, bool) {
    for {
        head := (*queueNode)(atomic.LoadPointer(&lfq.head))
        tail := (*queueNode)(atomic.LoadPointer(&lfq.tail))
        next := (*queueNode)(atomic.LoadPointer(&head.next))

        if head == (*queueNode)(atomic.LoadPointer(&lfq.head)) {
            if head == tail {
                if next == nil {
                    return nil, false // é˜Ÿåˆ—ä¸ºç©º
                }
                atomic.CompareAndSwapPointer(&lfq.tail, unsafe.Pointer(tail), unsafe.Pointer(next))
            } else {
                if next == nil {
                    continue
                }

                data := next.data
                if atomic.CompareAndSwapPointer(&lfq.head, unsafe.Pointer(head), unsafe.Pointer(next)) {
                    return data, true
                }
            }
        }
    }
}

// é”ç®¡ç†å™¨
type LockManager struct {
    locks map[string]*OptimizedRWMutex
    mutex sync.RWMutex
}

// åˆ›å»ºé”ç®¡ç†å™¨
func NewLockManager() *LockManager {
    return &LockManager{
        locks: make(map[string]*OptimizedRWMutex),
    }
}

// è·å–é”
func (lm *LockManager) GetLock(name string) *OptimizedRWMutex {
    lm.mutex.RLock()
    lock, exists := lm.locks[name]
    lm.mutex.RUnlock()

    if exists {
        return lock
    }

    lm.mutex.Lock()
    defer lm.mutex.Unlock()

    // åŒé‡æ£€æŸ¥
    if lock, exists := lm.locks[name]; exists {
        return lock
    }

    lock = NewOptimizedRWMutex()
    lm.locks[name] = lock
    return lock
}

// è·å–æ‰€æœ‰é”çš„ç»Ÿè®¡ä¿¡æ¯
func (lm *LockManager) GetAllStats() map[string]*RWMutexStats {
    lm.mutex.RLock()
    defer lm.mutex.RUnlock()

    stats := make(map[string]*RWMutexStats)
    for name, lock := range lm.locks {
        stats[name] = lock.Stats()
    }
    return stats
}
```

---

## ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

### è¿æ¥æ± ä¼˜åŒ–

```go
// æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
package database

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "sync/atomic"
    "time"

    "gorm.io/gorm"
)

// è¿æ¥æ± é…ç½®
type ConnectionPoolConfig struct {
    MaxOpenConns    int           `json:"max_open_conns"`
    MaxIdleConns    int           `json:"max_idle_conns"`
    ConnMaxLifetime time.Duration `json:"conn_max_lifetime"`
    ConnMaxIdleTime time.Duration `json:"conn_max_idle_time"`

    // é«˜çº§é…ç½®
    ReadTimeout     time.Duration `json:"read_timeout"`
    WriteTimeout    time.Duration `json:"write_timeout"`
    ConnectTimeout  time.Duration `json:"connect_timeout"`

    // ç›‘æ§é…ç½®
    EnableMetrics   bool          `json:"enable_metrics"`
    MetricsInterval time.Duration `json:"metrics_interval"`
}

// è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯
type ConnectionPoolStats struct {
    OpenConnections int           `json:"open_connections"`
    InUseConns      int           `json:"in_use_conns"`
    IdleConns       int           `json:"idle_conns"`
    WaitCount       int64         `json:"wait_count"`
    WaitDuration    time.Duration `json:"wait_duration"`
    MaxIdleClosed   int64         `json:"max_idle_closed"`
    MaxLifetimeClosed int64       `json:"max_lifetime_closed"`

    // æ€§èƒ½æŒ‡æ ‡
    AvgConnTime     time.Duration `json:"avg_conn_time"`
    AvgQueryTime    time.Duration `json:"avg_query_time"`
    QueryCount      int64         `json:"query_count"`
    ErrorCount      int64         `json:"error_count"`

    // å¥åº·çŠ¶æ€
    HealthScore     float64       `json:"health_score"`
    LastCheck       time.Time     `json:"last_check"`
}

// ä¼˜åŒ–çš„æ•°æ®åº“ç®¡ç†å™¨
type OptimizedDBManager struct {
    db              *gorm.DB
    config          *ConnectionPoolConfig
    stats           *ConnectionPoolStats
    queryStats      map[string]*QueryStats
    queryStatsMutex sync.RWMutex

    // ç›‘æ§
    monitoring      bool
    stopMonitoring  chan bool
    wg              sync.WaitGroup
}

// æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
type QueryStats struct {
    Count       int64         `json:"count"`
    TotalTime   time.Duration `json:"total_time"`
    AvgTime     time.Duration `json:"avg_time"`
    MinTime     time.Duration `json:"min_time"`
    MaxTime     time.Duration `json:"max_time"`
    ErrorCount  int64         `json:"error_count"`
    LastExecuted time.Time    `json:"last_executed"`
}

// åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åº“ç®¡ç†å™¨
func NewOptimizedDBManager(db *gorm.DB, config *ConnectionPoolConfig) *OptimizedDBManager {
    manager := &OptimizedDBManager{
        db:             db,
        config:         config,
        stats:          &ConnectionPoolStats{},
        queryStats:     make(map[string]*QueryStats),
        stopMonitoring: make(chan bool),
    }

    // é…ç½®è¿æ¥æ± 
    manager.configureConnectionPool()

    // å¯åŠ¨ç›‘æ§
    if config.EnableMetrics {
        manager.startMonitoring()
    }

    return manager
}

// é…ç½®è¿æ¥æ± 
func (odm *OptimizedDBManager) configureConnectionPool() {
    sqlDB, err := odm.db.DB()
    if err != nil {
        fmt.Printf("Failed to get sql.DB: %v\n", err)
        return
    }

    // è®¾ç½®è¿æ¥æ± å‚æ•°
    sqlDB.SetMaxOpenConns(odm.config.MaxOpenConns)
    sqlDB.SetMaxIdleConns(odm.config.MaxIdleConns)
    sqlDB.SetConnMaxLifetime(odm.config.ConnMaxLifetime)
    sqlDB.SetConnMaxIdleTime(odm.config.ConnMaxIdleTime)

    fmt.Printf("Database connection pool configured: MaxOpen=%d, MaxIdle=%d, MaxLifetime=%v\n",
        odm.config.MaxOpenConns, odm.config.MaxIdleConns, odm.config.ConnMaxLifetime)
}

// å¯åŠ¨ç›‘æ§
func (odm *OptimizedDBManager) startMonitoring() {
    odm.monitoring = true
    odm.wg.Add(1)

    go func() {
        defer odm.wg.Done()
        ticker := time.NewTicker(odm.config.MetricsInterval)
        defer ticker.Stop()

        for {
            select {
            case <-ticker.C:
                odm.collectStats()
            case <-odm.stopMonitoring:
                return
            }
        }
    }()
}

// åœæ­¢ç›‘æ§
func (odm *OptimizedDBManager) StopMonitoring() {
    if odm.monitoring {
        close(odm.stopMonitoring)
        odm.wg.Wait()
        odm.monitoring = false
    }
}

// æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
func (odm *OptimizedDBManager) collectStats() {
    sqlDB, err := odm.db.DB()
    if err != nil {
        return
    }

    dbStats := sqlDB.Stats()

    odm.stats.OpenConnections = dbStats.OpenConnections
    odm.stats.InUseConns = dbStats.InUse
    odm.stats.IdleConns = dbStats.Idle
    odm.stats.WaitCount = dbStats.WaitCount
    odm.stats.WaitDuration = dbStats.WaitDuration
    odm.stats.MaxIdleClosed = dbStats.MaxIdleClosed
    odm.stats.MaxLifetimeClosed = dbStats.MaxLifetimeClosed
    odm.stats.LastCheck = time.Now()

    // è®¡ç®—å¥åº·åˆ†æ•°
    odm.calculateHealthScore()
}

// è®¡ç®—å¥åº·åˆ†æ•°
func (odm *OptimizedDBManager) calculateHealthScore() {
    score := 100.0

    // è¿æ¥ä½¿ç”¨ç‡
    if odm.config.MaxOpenConns > 0 {
        usage := float64(odm.stats.InUseConns) / float64(odm.config.MaxOpenConns)
        if usage > 0.9 {
            score -= 20 // ä½¿ç”¨ç‡è¿‡é«˜æ‰£åˆ†
        } else if usage > 0.7 {
            score -= 10
        }
    }

    // ç­‰å¾…æ—¶é—´
    if odm.stats.WaitDuration > 100*time.Millisecond {
        score -= 15 // ç­‰å¾…æ—¶é—´è¿‡é•¿æ‰£åˆ†
    }

    // é”™è¯¯ç‡
    if odm.stats.QueryCount > 0 {
        errorRate := float64(odm.stats.ErrorCount) / float64(odm.stats.QueryCount)
        if errorRate > 0.05 {
            score -= 25 // é”™è¯¯ç‡è¿‡é«˜æ‰£åˆ†
        } else if errorRate > 0.01 {
            score -= 10
        }
    }

    if score < 0 {
        score = 0
    }

    odm.stats.HealthScore = score
}

// æ‰§è¡ŒæŸ¥è¯¢å¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯
func (odm *OptimizedDBManager) ExecuteQuery(query string, args ...interface{}) *gorm.DB {
    start := time.Now()

    // æ‰§è¡ŒæŸ¥è¯¢
    result := odm.db.Raw(query, args...)

    duration := time.Since(start)

    // è®°å½•ç»Ÿè®¡ä¿¡æ¯
    odm.recordQueryStats(query, duration, result.Error)

    return result
}

// è®°å½•æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
func (odm *OptimizedDBManager) recordQueryStats(query string, duration time.Duration, err error) {
    // ç®€åŒ–æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºkey
    queryKey := odm.normalizeQuery(query)

    odm.queryStatsMutex.Lock()
    defer odm.queryStatsMutex.Unlock()

    stats, exists := odm.queryStats[queryKey]
    if !exists {
        stats = &QueryStats{
            MinTime: duration,
            MaxTime: duration,
        }
        odm.queryStats[queryKey] = stats
    }

    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    stats.Count++
    stats.TotalTime += duration
    stats.AvgTime = stats.TotalTime / time.Duration(stats.Count)
    stats.LastExecuted = time.Now()

    if duration < stats.MinTime {
        stats.MinTime = duration
    }
    if duration > stats.MaxTime {
        stats.MaxTime = duration
    }

    if err != nil {
        stats.ErrorCount++
        atomic.AddInt64(&odm.stats.ErrorCount, 1)
    }

    atomic.AddInt64(&odm.stats.QueryCount, 1)
}

// æ ‡å‡†åŒ–æŸ¥è¯¢å­—ç¬¦ä¸²
func (odm *OptimizedDBManager) normalizeQuery(query string) string {
    // ç®€åŒ–å®ç°ï¼šç§»é™¤å‚æ•°ï¼Œåªä¿ç•™æŸ¥è¯¢ç»“æ„
    // å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ
    if len(query) > 100 {
        return query[:100] + "..."
    }
    return query
}

// è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯
func (odm *OptimizedDBManager) GetStats() *ConnectionPoolStats {
    return odm.stats
}

// è·å–æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
func (odm *OptimizedDBManager) GetQueryStats() map[string]*QueryStats {
    odm.queryStatsMutex.RLock()
    defer odm.queryStatsMutex.RUnlock()

    // å¤åˆ¶ç»Ÿè®¡ä¿¡æ¯
    result := make(map[string]*QueryStats)
    for k, v := range odm.queryStats {
        statsCopy := *v
        result[k] = &statsCopy
    }

    return result
}

// è·å–æ…¢æŸ¥è¯¢
func (odm *OptimizedDBManager) GetSlowQueries(threshold time.Duration) map[string]*QueryStats {
    odm.queryStatsMutex.RLock()
    defer odm.queryStatsMutex.RUnlock()

    slowQueries := make(map[string]*QueryStats)
    for query, stats := range odm.queryStats {
        if stats.AvgTime > threshold || stats.MaxTime > threshold*2 {
            statsCopy := *stats
            slowQueries[query] = &statsCopy
        }
    }

    return slowQueries
}

// å¥åº·æ£€æŸ¥
func (odm *OptimizedDBManager) HealthCheck() error {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    sqlDB, err := odm.db.DB()
    if err != nil {
        return fmt.Errorf("failed to get sql.DB: %w", err)
    }

    if err := sqlDB.PingContext(ctx); err != nil {
        return fmt.Errorf("database ping failed: %w", err)
    }

    // æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
    stats := sqlDB.Stats()
    if stats.OpenConnections == 0 {
        return fmt.Errorf("no open connections")
    }

    return nil
}

// ä¼˜åŒ–å»ºè®®
func (odm *OptimizedDBManager) GetOptimizationRecommendations() []string {
    recommendations := make([]string, 0)

    // åŸºäºç»Ÿè®¡ä¿¡æ¯æä¾›å»ºè®®
    if odm.stats.HealthScore < 70 {
        recommendations = append(recommendations, "Database health score is low, consider investigating connection pool settings")
    }

    if odm.stats.WaitDuration > 100*time.Millisecond {
        recommendations = append(recommendations, "High connection wait time detected, consider increasing MaxOpenConns")
    }

    if float64(odm.stats.InUseConns)/float64(odm.config.MaxOpenConns) > 0.8 {
        recommendations = append(recommendations, "Connection pool utilization is high, consider scaling up")
    }

    // æ£€æŸ¥æ…¢æŸ¥è¯¢
    slowQueries := odm.GetSlowQueries(1 * time.Second)
    if len(slowQueries) > 0 {
        recommendations = append(recommendations, fmt.Sprintf("Found %d slow queries, consider optimization", len(slowQueries)))
    }

    if odm.stats.ErrorCount > 0 && odm.stats.QueryCount > 0 {
        errorRate := float64(odm.stats.ErrorCount) / float64(odm.stats.QueryCount)
        if errorRate > 0.01 {
            recommendations = append(recommendations, fmt.Sprintf("High error rate detected: %.2f%%", errorRate*100))
        }
    }

    if len(recommendations) == 0 {
        recommendations = append(recommendations, "Database performance is within acceptable ranges")
    }

    return recommendations
}
```

---

## ğŸ’¾ ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

### å¤šçº§ç¼“å­˜æ¶æ„

```go
// å¤šçº§ç¼“å­˜ç®¡ç†å™¨
package cache

import (
    "context"
    "fmt"
    "sync"
    "time"

    "github.com/go-redis/redis/v8"
)

// ç¼“å­˜å±‚çº§å®šä¹‰
type CacheLevel int

const (
    L1Cache CacheLevel = iota // å†…å­˜ç¼“å­˜
    L2Cache                   // Redisç¼“å­˜
    L3Cache                   // åˆ†å¸ƒå¼ç¼“å­˜
)

// ç¼“å­˜é¡¹
type CacheItem struct {
    Key       string      `json:"key"`
    Value     interface{} `json:"value"`
    TTL       time.Duration `json:"ttl"`
    Level     CacheLevel  `json:"level"`
    CreatedAt time.Time   `json:"created_at"`
    AccessCount int64     `json:"access_count"`
    LastAccess time.Time  `json:"last_access"`
}

// å¤šçº§ç¼“å­˜ç®¡ç†å™¨
type MultiLevelCacheManager struct {
    l1Cache    *MemoryCache
    l2Cache    *RedisCache
    l3Cache    *DistributedCache
    config     *CacheConfig
    stats      *CacheStats
    mutex      sync.RWMutex
}

// ç¼“å­˜é…ç½®
type CacheConfig struct {
    L1 struct {
        MaxSize     int           `json:"max_size"`
        TTL         time.Duration `json:"ttl"`
        CleanupInterval time.Duration `json:"cleanup_interval"`
    } `json:"l1"`

    L2 struct {
        Addr        string        `json:"addr"`
        Password    string        `json:"password"`
        DB          int           `json:"db"`
        PoolSize    int           `json:"pool_size"`
        TTL         time.Duration `json:"ttl"`
    } `json:"l2"`

    L3 struct {
        Nodes       []string      `json:"nodes"`
        TTL         time.Duration `json:"ttl"`
    } `json:"l3"`

    // ç­–ç•¥é…ç½®
    WriteThrough   bool `json:"write_through"`
    WriteBack      bool `json:"write_back"`
    ReadThrough    bool `json:"read_through"`

    // æ€§èƒ½é…ç½®
    PrefetchEnabled bool          `json:"prefetch_enabled"`
    PrefetchRatio   float64       `json:"prefetch_ratio"`
    CompressionEnabled bool       `json:"compression_enabled"`
    SerializationFormat string    `json:"serialization_format"` // json, msgpack, protobuf
}

// ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
type CacheStats struct {
    L1Stats CacheLevelStats `json:"l1_stats"`
    L2Stats CacheLevelStats `json:"l2_stats"`
    L3Stats CacheLevelStats `json:"l3_stats"`

    TotalHits   int64   `json:"total_hits"`
    TotalMisses int64   `json:"total_misses"`
    HitRate     float64 `json:"hit_rate"`

    AvgLatency  time.Duration `json:"avg_latency"`
    Throughput  float64       `json:"throughput"`
}

// ç¼“å­˜å±‚çº§ç»Ÿè®¡
type CacheLevelStats struct {
    Hits        int64         `json:"hits"`
    Misses      int64         `json:"misses"`
    Sets        int64         `json:"sets"`
    Deletes     int64         `json:"deletes"`
    Size        int           `json:"size"`
    HitRate     float64       `json:"hit_rate"`
    AvgLatency  time.Duration `json:"avg_latency"`
}

// åˆ›å»ºå¤šçº§ç¼“å­˜ç®¡ç†å™¨
func NewMultiLevelCacheManager(config *CacheConfig) *MultiLevelCacheManager {
    manager := &MultiLevelCacheManager{
        config: config,
        stats:  &CacheStats{},
    }

    // åˆå§‹åŒ–å„çº§ç¼“å­˜
    manager.l1Cache = NewMemoryCache(config.L1.MaxSize, config.L1.TTL)
    manager.l2Cache = NewRedisCache(&RedisCacheConfig{
        Addr:     config.L2.Addr,
        Password: config.L2.Password,
        DB:       config.L2.DB,
        PoolSize: config.L2.PoolSize,
        TTL:      config.L2.TTL,
    })

    return manager
}

// è·å–ç¼“å­˜å€¼
func (mlcm *MultiLevelCacheManager) Get(ctx context.Context, key string) (interface{}, bool) {
    start := time.Now()
    defer func() {
        latency := time.Since(start)
        mlcm.updateLatencyStats(latency)
    }()

    // L1ç¼“å­˜æŸ¥æ‰¾
    if value, found := mlcm.l1Cache.Get(key); found {
        mlcm.stats.L1Stats.Hits++
        mlcm.stats.TotalHits++
        return value, true
    }
    mlcm.stats.L1Stats.Misses++

    // L2ç¼“å­˜æŸ¥æ‰¾
    if value, found := mlcm.l2Cache.Get(ctx, key); found {
        mlcm.stats.L2Stats.Hits++
        mlcm.stats.TotalHits++

        // å›å†™åˆ°L1ç¼“å­˜
        mlcm.l1Cache.Set(key, value, mlcm.config.L1.TTL)
        return value, true
    }
    mlcm.stats.L2Stats.Misses++

    // L3ç¼“å­˜æŸ¥æ‰¾ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    if mlcm.l3Cache != nil {
        if value, found := mlcm.l3Cache.Get(ctx, key); found {
            mlcm.stats.L3Stats.Hits++
            mlcm.stats.TotalHits++

            // å›å†™åˆ°L1å’ŒL2ç¼“å­˜
            mlcm.l1Cache.Set(key, value, mlcm.config.L1.TTL)
            mlcm.l2Cache.Set(ctx, key, value, mlcm.config.L2.TTL)
            return value, true
        }
        mlcm.stats.L3Stats.Misses++
    }

    mlcm.stats.TotalMisses++
    return nil, false
}

// è®¾ç½®ç¼“å­˜å€¼
func (mlcm *MultiLevelCacheManager) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error {
    // æ ¹æ®ç­–ç•¥å†³å®šå†™å…¥å“ªäº›å±‚çº§
    if mlcm.config.WriteThrough {
        // å†™ç©¿ç­–ç•¥ï¼šåŒæ—¶å†™å…¥æ‰€æœ‰å±‚çº§
        mlcm.l1Cache.Set(key, value, ttl)
        mlcm.stats.L1Stats.Sets++

        if err := mlcm.l2Cache.Set(ctx, key, value, ttl); err != nil {
            return fmt.Errorf("L2 cache set failed: %w", err)
        }
        mlcm.stats.L2Stats.Sets++

        if mlcm.l3Cache != nil {
            if err := mlcm.l3Cache.Set(ctx, key, value, ttl); err != nil {
                return fmt.Errorf("L3 cache set failed: %w", err)
            }
            mlcm.stats.L3Stats.Sets++
        }
    } else {
        // å†™å›ç­–ç•¥ï¼šåªå†™å…¥L1ç¼“å­˜
        mlcm.l1Cache.Set(key, value, ttl)
        mlcm.stats.L1Stats.Sets++
    }

    return nil
}

// åˆ é™¤ç¼“å­˜å€¼
func (mlcm *MultiLevelCacheManager) Delete(ctx context.Context, key string) error {
    // ä»æ‰€æœ‰å±‚çº§åˆ é™¤
    mlcm.l1Cache.Delete(key)
    mlcm.stats.L1Stats.Deletes++

    if err := mlcm.l2Cache.Delete(ctx, key); err != nil {
        return fmt.Errorf("L2 cache delete failed: %w", err)
    }
    mlcm.stats.L2Stats.Deletes++

    if mlcm.l3Cache != nil {
        if err := mlcm.l3Cache.Delete(ctx, key); err != nil {
            return fmt.Errorf("L3 cache delete failed: %w", err)
        }
        mlcm.stats.L3Stats.Deletes++
    }

    return nil
}

// æ›´æ–°å»¶è¿Ÿç»Ÿè®¡
func (mlcm *MultiLevelCacheManager) updateLatencyStats(latency time.Duration) {
    // ç®€åŒ–çš„å»¶è¿Ÿç»Ÿè®¡æ›´æ–°
    mlcm.mutex.Lock()
    defer mlcm.mutex.Unlock()

    // ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡
    alpha := 0.1
    if mlcm.stats.AvgLatency == 0 {
        mlcm.stats.AvgLatency = latency
    } else {
        mlcm.stats.AvgLatency = time.Duration(float64(mlcm.stats.AvgLatency)*(1-alpha) + float64(latency)*alpha)
    }
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (mlcm *MultiLevelCacheManager) GetStats() *CacheStats {
    mlcm.mutex.RLock()
    defer mlcm.mutex.RUnlock()

    stats := *mlcm.stats

    // è®¡ç®—å‘½ä¸­ç‡
    totalOps := stats.TotalHits + stats.TotalMisses
    if totalOps > 0 {
        stats.HitRate = float64(stats.TotalHits) / float64(totalOps) * 100
    }

    // è®¡ç®—å„å±‚çº§å‘½ä¸­ç‡
    l1Ops := stats.L1Stats.Hits + stats.L1Stats.Misses
    if l1Ops > 0 {
        stats.L1Stats.HitRate = float64(stats.L1Stats.Hits) / float64(l1Ops) * 100
    }

    l2Ops := stats.L2Stats.Hits + stats.L2Stats.Misses
    if l2Ops > 0 {
        stats.L2Stats.HitRate = float64(stats.L2Stats.Hits) / float64(l2Ops) * 100
    }

    return &stats
}

// å†…å­˜ç¼“å­˜å®ç°
type MemoryCache struct {
    data      map[string]*CacheItem
    maxSize   int
    defaultTTL time.Duration
    mutex     sync.RWMutex
}

func NewMemoryCache(maxSize int, defaultTTL time.Duration) *MemoryCache {
    return &MemoryCache{
        data:       make(map[string]*CacheItem),
        maxSize:    maxSize,
        defaultTTL: defaultTTL,
    }
}

func (mc *MemoryCache) Get(key string) (interface{}, bool) {
    mc.mutex.RLock()
    defer mc.mutex.RUnlock()

    item, exists := mc.data[key]
    if !exists {
        return nil, false
    }

    // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    if time.Since(item.CreatedAt) > item.TTL {
        delete(mc.data, key)
        return nil, false
    }

    item.AccessCount++
    item.LastAccess = time.Now()
    return item.Value, true
}

func (mc *MemoryCache) Set(key string, value interface{}, ttl time.Duration) {
    mc.mutex.Lock()
    defer mc.mutex.Unlock()

    // æ£€æŸ¥å®¹é‡é™åˆ¶
    if len(mc.data) >= mc.maxSize {
        mc.evictLRU()
    }

    if ttl == 0 {
        ttl = mc.defaultTTL
    }

    mc.data[key] = &CacheItem{
        Key:       key,
        Value:     value,
        TTL:       ttl,
        CreatedAt: time.Now(),
        LastAccess: time.Now(),
    }
}

func (mc *MemoryCache) Delete(key string) {
    mc.mutex.Lock()
    defer mc.mutex.Unlock()
    delete(mc.data, key)
}

// LRUæ·˜æ±°
func (mc *MemoryCache) evictLRU() {
    var oldestKey string
    var oldestTime time.Time

    for key, item := range mc.data {
        if oldestKey == "" || item.LastAccess.Before(oldestTime) {
            oldestKey = key
            oldestTime = item.LastAccess
        }
    }

    if oldestKey != "" {
        delete(mc.data, oldestKey)
    }
}

// Redisç¼“å­˜å®ç°
type RedisCache struct {
    client *redis.Client
    config *RedisCacheConfig
}

type RedisCacheConfig struct {
    Addr     string
    Password string
    DB       int
    PoolSize int
    TTL      time.Duration
}

func NewRedisCache(config *RedisCacheConfig) *RedisCache {
    client := redis.NewClient(&redis.Options{
        Addr:     config.Addr,
        Password: config.Password,
        DB:       config.DB,
        PoolSize: config.PoolSize,
    })

    return &RedisCache{
        client: client,
        config: config,
    }
}

func (rc *RedisCache) Get(ctx context.Context, key string) (interface{}, bool) {
    result, err := rc.client.Get(ctx, key).Result()
    if err != nil {
        if err == redis.Nil {
            return nil, false
        }
        return nil, false
    }

    return result, true
}

func (rc *RedisCache) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error {
    if ttl == 0 {
        ttl = rc.config.TTL
    }

    return rc.client.Set(ctx, key, value, ttl).Err()
}

func (rc *RedisCache) Delete(ctx context.Context, key string) error {
    return rc.client.Del(ctx, key).Err()
}

// åˆ†å¸ƒå¼ç¼“å­˜å®ç°ï¼ˆç®€åŒ–ï¼‰
type DistributedCache struct {
    // å®ç°åˆ†å¸ƒå¼ç¼“å­˜é€»è¾‘
}

func (dc *DistributedCache) Get(ctx context.Context, key string) (interface{}, bool) {
    // å®ç°åˆ†å¸ƒå¼ç¼“å­˜è·å–é€»è¾‘
    return nil, false
}

func (dc *DistributedCache) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error {
    // å®ç°åˆ†å¸ƒå¼ç¼“å­˜è®¾ç½®é€»è¾‘
    return nil
}

func (dc *DistributedCache) Delete(ctx context.Context, key string) error {
    // å®ç°åˆ†å¸ƒå¼ç¼“å­˜åˆ é™¤é€»è¾‘
    return nil
}
```

---

## ğŸŒ ç½‘ç»œæ€§èƒ½ä¼˜åŒ–

### HTTP/2å’Œè¿æ¥å¤ç”¨

```go
// ç½‘ç»œæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨
package network

import (
    "compress/gzip"
    "context"
    "crypto/tls"
    "fmt"
    "io"
    "net"
    "net/http"
    "sync"
    "time"

    "golang.org/x/net/http2"
)

// ç½‘ç»œä¼˜åŒ–é…ç½®
type NetworkOptimizationConfig struct {
    // HTTP/2é…ç½®
    HTTP2 struct {
        Enabled           bool `json:"enabled"`
        MaxConcurrentStreams uint32 `json:"max_concurrent_streams"`
        MaxFrameSize      uint32 `json:"max_frame_size"`
        MaxHeaderListSize uint32 `json:"max_header_list_size"`
        IdleTimeout       time.Duration `json:"idle_timeout"`
    } `json:"http2"`

    // è¿æ¥æ± é…ç½®
    ConnectionPool struct {
        MaxIdleConns        int           `json:"max_idle_conns"`
        MaxIdleConnsPerHost int           `json:"max_idle_conns_per_host"`
        MaxConnsPerHost     int           `json:"max_conns_per_host"`
        IdleConnTimeout     time.Duration `json:"idle_conn_timeout"`
        KeepAlive           time.Duration `json:"keep_alive"`
        TLSHandshakeTimeout time.Duration `json:"tls_handshake_timeout"`
    } `json:"connection_pool"`

    // å‹ç¼©é…ç½®
    Compression struct {
        Enabled bool     `json:"enabled"`
        Level   int      `json:"level"`
        Types   []string `json:"types"`
    } `json:"compression"`

    // ç¼“å­˜é…ç½®
    Cache struct {
        Enabled    bool          `json:"enabled"`
        MaxAge     time.Duration `json:"max_age"`
        ETagEnabled bool         `json:"etag_enabled"`
    } `json:"cache"`
}

// ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯
type OptimizedHTTPClient struct {
    client *http.Client
    config *NetworkOptimizationConfig
    stats  *NetworkStats
    mutex  sync.RWMutex
}

// ç½‘ç»œç»Ÿè®¡ä¿¡æ¯
type NetworkStats struct {
    RequestCount    int64         `json:"request_count"`
    ResponseTime    time.Duration `json:"response_time"`
    BytesSent       int64         `json:"bytes_sent"`
    BytesReceived   int64         `json:"bytes_received"`
    ConnectionReuse int64         `json:"connection_reuse"`
    CompressionRatio float64      `json:"compression_ratio"`
    ErrorCount      int64         `json:"error_count"`
}

// åˆ›å»ºä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯
func NewOptimizedHTTPClient(config *NetworkOptimizationConfig) *OptimizedHTTPClient {
    // åˆ›å»ºè‡ªå®šä¹‰Transport
    transport := &http.Transport{
        DialContext: (&net.Dialer{
            Timeout:   30 * time.Second,
            KeepAlive: config.ConnectionPool.KeepAlive,
        }).DialContext,

        MaxIdleConns:        config.ConnectionPool.MaxIdleConns,
        MaxIdleConnsPerHost: config.ConnectionPool.MaxIdleConnsPerHost,
        MaxConnsPerHost:     config.ConnectionPool.MaxConnsPerHost,
        IdleConnTimeout:     config.ConnectionPool.IdleConnTimeout,
        TLSHandshakeTimeout: config.ConnectionPool.TLSHandshakeTimeout,

        // å¯ç”¨HTTP/2
        ForceAttemptHTTP2: config.HTTP2.Enabled,

        // TLSé…ç½®
        TLSClientConfig: &tls.Config{
            InsecureSkipVerify: false,
            MinVersion:         tls.VersionTLS12,
        },
    }

    // é…ç½®HTTP/2
    if config.HTTP2.Enabled {
        http2.ConfigureTransport(transport)
    }

    client := &http.Client{
        Transport: transport,
        Timeout:   30 * time.Second,
    }

    return &OptimizedHTTPClient{
        client: client,
        config: config,
        stats:  &NetworkStats{},
    }
}

// æ‰§è¡ŒHTTPè¯·æ±‚
func (ohc *OptimizedHTTPClient) Do(req *http.Request) (*http.Response, error) {
    start := time.Now()

    // æ·»åŠ å‹ç¼©æ”¯æŒ
    if ohc.config.Compression.Enabled {
        req.Header.Set("Accept-Encoding", "gzip, deflate")
    }

    // æ·»åŠ ç¼“å­˜æ§åˆ¶
    if ohc.config.Cache.Enabled {
        req.Header.Set("Cache-Control", fmt.Sprintf("max-age=%d", int(ohc.config.Cache.MaxAge.Seconds())))
    }

    // æ‰§è¡Œè¯·æ±‚
    resp, err := ohc.client.Do(req)
    if err != nil {
        ohc.stats.ErrorCount++
        return nil, err
    }

    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    duration := time.Since(start)
    ohc.updateStats(req, resp, duration)

    return resp, nil
}

// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
func (ohc *OptimizedHTTPClient) updateStats(req *http.Request, resp *http.Response, duration time.Duration) {
    ohc.mutex.Lock()
    defer ohc.mutex.Unlock()

    ohc.stats.RequestCount++
    ohc.stats.ResponseTime = duration

    // è®¡ç®—ä¼ è¾“å­—èŠ‚æ•°
    if req.ContentLength > 0 {
        ohc.stats.BytesSent += req.ContentLength
    }
    if resp.ContentLength > 0 {
        ohc.stats.BytesReceived += resp.ContentLength
    }

    // æ£€æŸ¥è¿æ¥å¤ç”¨
    if resp.Header.Get("Connection") == "keep-alive" {
        ohc.stats.ConnectionReuse++
    }

    // æ£€æŸ¥å‹ç¼©
    if resp.Header.Get("Content-Encoding") == "gzip" {
        // ç®€åŒ–çš„å‹ç¼©æ¯”è®¡ç®—
        ohc.stats.CompressionRatio = 0.7 // å‡è®¾70%çš„å‹ç¼©ç‡
    }
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (ohc *OptimizedHTTPClient) GetStats() *NetworkStats {
    ohc.mutex.RLock()
    defer ohc.mutex.RUnlock()

    stats := *ohc.stats
    return &stats
}

// å‹ç¼©ä¸­é—´ä»¶
func CompressionMiddleware(level int) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            // æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ”¯æŒgzip
            if !strings.Contains(r.Header.Get("Accept-Encoding"), "gzip") {
                next.ServeHTTP(w, r)
                return
            }

            // åˆ›å»ºgzip writer
            gz, err := gzip.NewWriterLevel(w, level)
            if err != nil {
                next.ServeHTTP(w, r)
                return
            }
            defer gz.Close()

            // è®¾ç½®å“åº”å¤´
            w.Header().Set("Content-Encoding", "gzip")
            w.Header().Set("Vary", "Accept-Encoding")

            // åŒ…è£…ResponseWriter
            gzw := &gzipResponseWriter{
                ResponseWriter: w,
                Writer:         gz,
            }

            next.ServeHTTP(gzw, r)
        })
    }
}

// gzipå“åº”å†™å…¥å™¨
type gzipResponseWriter struct {
    http.ResponseWriter
    io.Writer
}

func (grw *gzipResponseWriter) Write(data []byte) (int, error) {
    return grw.Writer.Write(data)
}

// HTTP/2æœåŠ¡å™¨é…ç½®
func ConfigureHTTP2Server(server *http.Server, config *NetworkOptimizationConfig) error {
    if !config.HTTP2.Enabled {
        return nil
    }

    // é…ç½®HTTP/2
    http2Server := &http2.Server{
        MaxConcurrentStreams: config.HTTP2.MaxConcurrentStreams,
        MaxReadFrameSize:     config.HTTP2.MaxFrameSize,
        MaxUploadBufferPerConnection: 1 << 20, // 1MB
        MaxUploadBufferPerStream:     1 << 16, // 64KB
        IdleTimeout:                  config.HTTP2.IdleTimeout,
    }

    return http2.ConfigureServer(server, http2Server)
}

// è¿æ¥æ± ç›‘æ§
type ConnectionPoolMonitor struct {
    transport *http.Transport
    stats     *ConnectionPoolStats
    mutex     sync.RWMutex
}

type ConnectionPoolStats struct {
    IdleConnections     int `json:"idle_connections"`
    ActiveConnections   int `json:"active_connections"`
    TotalConnections    int `json:"total_connections"`
    ConnectionsCreated  int64 `json:"connections_created"`
    ConnectionsReused   int64 `json:"connections_reused"`
    ConnectionsClosed   int64 `json:"connections_closed"`
}

func NewConnectionPoolMonitor(transport *http.Transport) *ConnectionPoolMonitor {
    return &ConnectionPoolMonitor{
        transport: transport,
        stats:     &ConnectionPoolStats{},
    }
}

func (cpm *ConnectionPoolMonitor) GetStats() *ConnectionPoolStats {
    cpm.mutex.RLock()
    defer cpm.mutex.RUnlock()

    // æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨åå°„æˆ–å…¶ä»–æ–¹æ³•æ¥è·å–transportçš„å†…éƒ¨çŠ¶æ€
    // è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°
    stats := *cpm.stats
    return &stats
}

// ç½‘ç»œæ€§èƒ½åˆ†æå™¨
type NetworkPerformanceAnalyzer struct {
    metrics map[string]*NetworkMetric
    mutex   sync.RWMutex
}

type NetworkMetric struct {
    Name         string        `json:"name"`
    Count        int64         `json:"count"`
    TotalTime    time.Duration `json:"total_time"`
    MinTime      time.Duration `json:"min_time"`
    MaxTime      time.Duration `json:"max_time"`
    AvgTime      time.Duration `json:"avg_time"`
    ErrorCount   int64         `json:"error_count"`
    LastMeasured time.Time     `json:"last_measured"`
}

func NewNetworkPerformanceAnalyzer() *NetworkPerformanceAnalyzer {
    return &NetworkPerformanceAnalyzer{
        metrics: make(map[string]*NetworkMetric),
    }
}

func (npa *NetworkPerformanceAnalyzer) RecordRequest(name string, duration time.Duration, err error) {
    npa.mutex.Lock()
    defer npa.mutex.Unlock()

    metric, exists := npa.metrics[name]
    if !exists {
        metric = &NetworkMetric{
            Name:    name,
            MinTime: duration,
            MaxTime: duration,
        }
        npa.metrics[name] = metric
    }

    metric.Count++
    metric.TotalTime += duration
    metric.AvgTime = metric.TotalTime / time.Duration(metric.Count)
    metric.LastMeasured = time.Now()

    if duration < metric.MinTime {
        metric.MinTime = duration
    }
    if duration > metric.MaxTime {
        metric.MaxTime = duration
    }

    if err != nil {
        metric.ErrorCount++
    }
}

func (npa *NetworkPerformanceAnalyzer) GetMetrics() map[string]*NetworkMetric {
    npa.mutex.RLock()
    defer npa.mutex.RUnlock()

    result := make(map[string]*NetworkMetric)
    for k, v := range npa.metrics {
        metricCopy := *v
        result[k] = &metricCopy
    }

    return result
}

func (npa *NetworkPerformanceAnalyzer) GetSlowRequests(threshold time.Duration) map[string]*NetworkMetric {
    npa.mutex.RLock()
    defer npa.mutex.RUnlock()

    slowRequests := make(map[string]*NetworkMetric)
    for name, metric := range npa.metrics {
        if metric.AvgTime > threshold || metric.MaxTime > threshold*2 {
            metricCopy := *metric
            slowRequests[name] = &metricCopy
        }
    }

    return slowRequests
}
```

---

## ğŸ¯ é¢è¯•å¸¸è€ƒçŸ¥è¯†ç‚¹

### 1. Goæ€§èƒ½åˆ†æå·¥å…·

**Q: è¯·è¯¦ç»†ä»‹ç»Goè¯­è¨€çš„æ€§èƒ½åˆ†æå·¥å…·pprofçš„ä½¿ç”¨æ–¹æ³•ï¼Ÿ**

**A: pprofæ˜¯Goè¯­è¨€å†…ç½®çš„æ€§èƒ½åˆ†æå·¥å…·ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š**

1. **CPUæ€§èƒ½åˆ†æ**
```go
// å¯åŠ¨CPUæ€§èƒ½åˆ†æ
import _ "net/http/pprof"

// åœ¨ä»£ç ä¸­å¯åŠ¨
f, err := os.Create("cpu.prof")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

if err := pprof.StartCPUProfile(f); err != nil {
    log.Fatal(err)
}
defer pprof.StopCPUProfile()

// åˆ†æå‘½ä»¤
// go tool pprof cpu.prof
// (pprof) top10
// (pprof) list functionName
// (pprof) web
```

2. **å†…å­˜æ€§èƒ½åˆ†æ**
```go
// å†…å­˜åˆ†æ
runtime.GC() // å¼ºåˆ¶GC
f, err := os.Create("mem.prof")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

if err := pprof.WriteHeapProfile(f); err != nil {
    log.Fatal(err)
}

// åˆ†æå‘½ä»¤
// go tool pprof mem.prof
// (pprof) top10
// (pprof) list functionName
```

3. **Goroutineåˆ†æ**
```go
// Goroutineåˆ†æ
pprof.Lookup("goroutine").WriteTo(os.Stdout, 1)

// é€šè¿‡HTTPæ¥å£
// http://localhost:6060/debug/pprof/goroutine?debug=1
```

**Q: å¦‚ä½•ä½¿ç”¨go traceå·¥å…·åˆ†æç¨‹åºæ‰§è¡Œï¼Ÿ**

**A: go traceå·¥å…·ç”¨äºåˆ†æç¨‹åºçš„æ‰§è¡Œè½¨è¿¹ï¼š**

```go
// å¯åŠ¨trace
f, err := os.Create("trace.out")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

if err := trace.Start(f); err != nil {
    log.Fatal(err)
}
defer trace.Stop()

// åˆ†æå‘½ä»¤
// go tool trace trace.out
// åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹è¯¦ç»†çš„æ‰§è¡Œæ—¶é—´çº¿
```

### 2. å†…å­˜ä¼˜åŒ–ç­–ç•¥

**Q: Goè¯­è¨€ä¸­å¦‚ä½•æ£€æµ‹å’Œé¿å…å†…å­˜æ³„æ¼ï¼Ÿ**

**A: å†…å­˜æ³„æ¼æ£€æµ‹å’Œé¢„é˜²ç­–ç•¥ï¼š**

1. **å¸¸è§å†…å­˜æ³„æ¼åœºæ™¯**
```go
// 1. Goroutineæ³„æ¼
func badGoroutine() {
    go func() {
        for {
            // æ²¡æœ‰é€€å‡ºæ¡ä»¶çš„æ— é™å¾ªç¯
            time.Sleep(1 * time.Second)
        }
    }()
}

// æ­£ç¡®åšæ³•
func goodGoroutine(ctx context.Context) {
    go func() {
        for {
            select {
            case <-ctx.Done():
                return
            default:
                time.Sleep(1 * time.Second)
            }
        }
    }()
}

// 2. åˆ‡ç‰‡å®¹é‡æ³„æ¼
func badSlice() []int {
    s := make([]int, 1000000)
    return s[:3] // ä»ç„¶å¼•ç”¨æ•´ä¸ªåº•å±‚æ•°ç»„
}

// æ­£ç¡®åšæ³•
func goodSlice() []int {
    s := make([]int, 1000000)
    result := make([]int, 3)
    copy(result, s[:3])
    return result
}

// 3. å®šæ—¶å™¨æ³„æ¼
func badTimer() {
    timer := time.NewTimer(1 * time.Hour)
    // å¿˜è®°åœæ­¢å®šæ—¶å™¨
}

// æ­£ç¡®åšæ³•
func goodTimer() {
    timer := time.NewTimer(1 * time.Hour)
    defer timer.Stop()
}
```

2. **å†…å­˜æ³„æ¼æ£€æµ‹æ–¹æ³•**
```go
// ä½¿ç”¨runtime.ReadMemStatsç›‘æ§
var m runtime.MemStats
runtime.ReadMemStats(&m)
fmt.Printf("Alloc = %d KB", bToKb(m.Alloc))
fmt.Printf("TotalAlloc = %d KB", bToKb(m.TotalAlloc))
fmt.Printf("Sys = %d KB", bToKb(m.Sys))
fmt.Printf("NumGC = %v\n", m.NumGC)

// ä½¿ç”¨pprofåˆ†æå †å†…å­˜
go tool pprof http://localhost:6060/debug/pprof/heap
```

**Q: å¦‚ä½•ä¼˜åŒ–Goç¨‹åºçš„GCæ€§èƒ½ï¼Ÿ**

**A: GCä¼˜åŒ–ç­–ç•¥ï¼š**

1. **è°ƒæ•´GOGCå‚æ•°**
```go
// è®¾ç½®GCç›®æ ‡ç™¾åˆ†æ¯”
debug.SetGCPercent(200) // é»˜è®¤100

// ç¯å¢ƒå˜é‡è®¾ç½®
// GOGC=200 go run main.go
```

2. **å‡å°‘å†…å­˜åˆ†é…**
```go
// ä½¿ç”¨å¯¹è±¡æ± 
var bufferPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 1024)
    },
}

func useBuffer() {
    buf := bufferPool.Get().([]byte)
    defer bufferPool.Put(buf)
    // ä½¿ç”¨buf
}

// é¢„åˆ†é…åˆ‡ç‰‡å®¹é‡
func preAllocate() {
    // ä¸å¥½çš„åšæ³•
    var s []int
    for i := 0; i < 1000; i++ {
        s = append(s, i) // å¤šæ¬¡é‡æ–°åˆ†é…
    }

    // å¥½çš„åšæ³•
    s := make([]int, 0, 1000) // é¢„åˆ†é…å®¹é‡
    for i := 0; i < 1000; i++ {
        s = append(s, i)
    }
}
```

### 3. å¹¶å‘æ€§èƒ½ä¼˜åŒ–

**Q: å¦‚ä½•è®¾è®¡é«˜æ€§èƒ½çš„Goroutineæ± ï¼Ÿ**

**A: Goroutineæ± è®¾è®¡è¦ç‚¹ï¼š**

1. **åŠ¨æ€è°ƒæ•´æ± å¤§å°**
```go
type WorkerPool struct {
    workers    int
    maxWorkers int
    taskQueue  chan Task
    wg         sync.WaitGroup
}

func (wp *WorkerPool) adjustWorkers() {
    queueLen := len(wp.taskQueue)

    // é˜Ÿåˆ—ç§¯å‹è¿‡å¤šï¼Œå¢åŠ worker
    if queueLen > cap(wp.taskQueue)/2 && wp.workers < wp.maxWorkers {
        wp.addWorker()
    }

    // é˜Ÿåˆ—ç©ºé—²ï¼Œå‡å°‘worker
    if queueLen == 0 && wp.workers > wp.minWorkers {
        wp.removeWorker()
    }
}
```

2. **é¿å…Goroutineæ³„æ¼**
```go
func (wp *WorkerPool) worker(ctx context.Context) {
    defer wp.wg.Done()

    for {
        select {
        case task := <-wp.taskQueue:
            task.Execute()
        case <-ctx.Done():
            return // ä¼˜é›…é€€å‡º
        }
    }
}
```

**Q: Channelçš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§æœ‰å“ªäº›ï¼Ÿ**

**A: Channelä¼˜åŒ–ç­–ç•¥ï¼š**

1. **é€‰æ‹©åˆé€‚çš„ç¼“å†²åŒºå¤§å°**
```go
// æ— ç¼“å†²channel - åŒæ­¥é€šä¿¡
ch := make(chan int)

// æœ‰ç¼“å†²channel - å¼‚æ­¥é€šä¿¡
ch := make(chan int, 100)

// æ ¹æ®ç”Ÿäº§è€…æ¶ˆè´¹è€…é€Ÿåº¦è°ƒæ•´ç¼“å†²åŒºå¤§å°
```

2. **ä½¿ç”¨selecté¿å…é˜»å¡**
```go
// éé˜»å¡å‘é€
select {
case ch <- data:
    // å‘é€æˆåŠŸ
default:
    // å‘é€å¤±è´¥ï¼Œå¤„ç†é€»è¾‘
}

// å¸¦è¶…æ—¶çš„æ“ä½œ
select {
case result := <-ch:
    // æ¥æ”¶æˆåŠŸ
case <-time.After(1 * time.Second):
    // è¶…æ—¶å¤„ç†
}
```

### 4. æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

**Q: å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± çš„æ€§èƒ½ï¼Ÿ**

**A: æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–ï¼š**

1. **åˆç†è®¾ç½®è¿æ¥æ± å‚æ•°**
```go
// è®¾ç½®æœ€å¤§è¿æ¥æ•°
db.SetMaxOpenConns(25)

// è®¾ç½®æœ€å¤§ç©ºé—²è¿æ¥æ•°
db.SetMaxIdleConns(25)

// è®¾ç½®è¿æ¥æœ€å¤§ç”Ÿå­˜æ—¶é—´
db.SetConnMaxLifetime(5 * time.Minute)

// è®¾ç½®è¿æ¥æœ€å¤§ç©ºé—²æ—¶é—´
db.SetConnMaxIdleTime(5 * time.Minute)
```

2. **ç›‘æ§è¿æ¥æ± çŠ¶æ€**
```go
stats := db.Stats()
fmt.Printf("Open connections: %d\n", stats.OpenConnections)
fmt.Printf("In use: %d\n", stats.InUse)
fmt.Printf("Idle: %d\n", stats.Idle)
fmt.Printf("Wait count: %d\n", stats.WaitCount)
fmt.Printf("Wait duration: %v\n", stats.WaitDuration)
```

**Q: å¦‚ä½•è¿›è¡ŒSQLæŸ¥è¯¢ä¼˜åŒ–ï¼Ÿ**

**A: SQLæŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥ï¼š**

1. **ä½¿ç”¨é¢„ç¼–è¯‘è¯­å¥**
```go
// é¢„ç¼–è¯‘è¯­å¥
stmt, err := db.Prepare("SELECT * FROM users WHERE id = ?")
if err != nil {
    log.Fatal(err)
}
defer stmt.Close()

// é‡å¤ä½¿ç”¨
for _, id := range userIDs {
    rows, err := stmt.Query(id)
    // å¤„ç†ç»“æœ
}
```

2. **æ‰¹é‡æ“ä½œ**
```go
// æ‰¹é‡æ’å…¥
tx, err := db.Begin()
if err != nil {
    log.Fatal(err)
}

stmt, err := tx.Prepare("INSERT INTO users (name, email) VALUES (?, ?)")
if err != nil {
    log.Fatal(err)
}

for _, user := range users {
    _, err := stmt.Exec(user.Name, user.Email)
    if err != nil {
        tx.Rollback()
        log.Fatal(err)
    }
}

stmt.Close()
tx.Commit()
```

### 5. ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

**Q: å¦‚ä½•è®¾è®¡å¤šçº§ç¼“å­˜ç³»ç»Ÿï¼Ÿ**

**A: å¤šçº§ç¼“å­˜è®¾è®¡åŸåˆ™ï¼š**

1. **ç¼“å­˜å±‚çº§è®¾è®¡**
```go
// L1: å†…å­˜ç¼“å­˜ (æœ€å¿«ï¼Œå®¹é‡å°)
// L2: Redisç¼“å­˜ (å¿«ï¼Œå®¹é‡ä¸­ç­‰)
// L3: åˆ†å¸ƒå¼ç¼“å­˜ (ç›¸å¯¹æ…¢ï¼Œå®¹é‡å¤§)

type MultiLevelCache struct {
    l1 *MemoryCache
    l2 *RedisCache
    l3 *DistributedCache
}

func (mlc *MultiLevelCache) Get(key string) (interface{}, bool) {
    // ä¾æ¬¡æŸ¥æ‰¾å„çº§ç¼“å­˜
    if value, found := mlc.l1.Get(key); found {
        return value, true
    }

    if value, found := mlc.l2.Get(key); found {
        mlc.l1.Set(key, value) // å›å†™åˆ°L1
        return value, true
    }

    if value, found := mlc.l3.Get(key); found {
        mlc.l1.Set(key, value) // å›å†™åˆ°L1
        mlc.l2.Set(key, value) // å›å†™åˆ°L2
        return value, true
    }

    return nil, false
}
```

2. **ç¼“å­˜ç­–ç•¥é€‰æ‹©**
```go
// å†™ç©¿ç­–ç•¥ (Write-Through)
func (cache *Cache) SetWriteThrough(key string, value interface{}) {
    cache.Set(key, value)
    database.Save(key, value) // åŒæ—¶å†™å…¥æ•°æ®åº“
}

// å†™å›ç­–ç•¥ (Write-Back)
func (cache *Cache) SetWriteBack(key string, value interface{}) {
    cache.Set(key, value)
    cache.markDirty(key) // æ ‡è®°ä¸ºè„æ•°æ®ï¼Œç¨åå†™å…¥æ•°æ®åº“
}

// å†™ç»•ç­–ç•¥ (Write-Around)
func (cache *Cache) SetWriteAround(key string, value interface{}) {
    database.Save(key, value) // ç›´æ¥å†™å…¥æ•°æ®åº“ï¼Œä¸æ›´æ–°ç¼“å­˜
}
```

---

## ğŸ‹ï¸ å®æˆ˜ç»ƒä¹ é¢˜

### ç»ƒä¹ 1: æ€§èƒ½åˆ†æä¸ä¼˜åŒ–

**é¢˜ç›®**: ç»™å®šä¸€ä¸ªGoç¨‹åºï¼Œä½¿ç”¨pprofå·¥å…·åˆ†ææ€§èƒ½ç“¶é¢ˆå¹¶è¿›è¡Œä¼˜åŒ–

**é—®é¢˜ä»£ç **:
```go
func inefficientFunction() {
    var result []string
    for i := 0; i < 100000; i++ {
        result = append(result, fmt.Sprintf("item_%d", i))
    }

    // å­—ç¬¦ä¸²æ‹¼æ¥
    var combined string
    for _, s := range result {
        combined += s + ","
    }

    // é¢‘ç¹çš„mapæ“ä½œ
    m := make(map[string]int)
    for i, s := range result {
        m[s] = i
    }
}
```

**è¦æ±‚**:
1. ä½¿ç”¨pprofåˆ†æCPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
2. è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
3. æä¾›ä¼˜åŒ–æ–¹æ¡ˆ
4. å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½æ•°æ®

**ä¼˜åŒ–æ–¹å‘**:
- é¢„åˆ†é…åˆ‡ç‰‡å®¹é‡
- ä½¿ç”¨strings.Builderè¿›è¡Œå­—ç¬¦ä¸²æ‹¼æ¥
- ä¼˜åŒ–mapæ“ä½œ
- å‡å°‘å†…å­˜åˆ†é…

### ç»ƒä¹ 2: å¹¶å‘æ€§èƒ½ä¼˜åŒ–

**é¢˜ç›®**: è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„ä»»åŠ¡å¤„ç†ç³»ç»Ÿ

**è¦æ±‚**:
1. å®ç°ä¸€ä¸ªå¯åŠ¨æ€è°ƒæ•´å¤§å°çš„Goroutineæ± 
2. æ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§
3. å®ç°ä»»åŠ¡è¶…æ—¶æœºåˆ¶
4. æä¾›è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
5. å¤„ç†ä¼˜é›…å…³é—­

**æŠ€æœ¯è¦ç‚¹**:
- Goroutineæ± ç®¡ç†
- Channelä¼˜åŒ–
- ä¸Šä¸‹æ–‡ä¼ é€’
- æ€§èƒ½ç›‘æ§
- èµ„æºæ¸…ç†

### ç»ƒä¹ 3: ç¼“å­˜ç³»ç»Ÿè®¾è®¡

**é¢˜ç›®**: å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤šçº§ç¼“å­˜ç³»ç»Ÿ

**è¦æ±‚**:
1. æ”¯æŒL1(å†…å­˜)ã€L2(Redis)ã€L3(åˆ†å¸ƒå¼)ä¸‰çº§ç¼“å­˜
2. å®ç°LRUã€LFUç­‰æ·˜æ±°ç­–ç•¥
3. æ”¯æŒç¼“å­˜é¢„çƒ­å’Œå¤±æ•ˆ
4. å®ç°ç¼“å­˜ç©¿é€ã€é›ªå´©é˜²æŠ¤
5. æä¾›ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡

**æŠ€æœ¯è¦ç‚¹**:
- å¤šçº§ç¼“å­˜æ¶æ„
- ç¼“å­˜ç­–ç•¥å®ç°
- å¹¶å‘å®‰å…¨
- æ€§èƒ½ç›‘æ§
- æ•…éšœå¤„ç†

---

## ğŸ“š æœ¬ç« æ€»ç»“

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥æŒæ¡äº†Goç¨‹åºæ€§èƒ½ä¼˜åŒ–çš„å®Œæ•´ä½“ç³»ï¼š

### ğŸ¯ æ ¸å¿ƒæ”¶è·

1. **æ€§èƒ½åˆ†æå·¥å…·** ğŸ”
   - æŒæ¡äº†pprofã€traceç­‰æ€§èƒ½åˆ†æå·¥å…·çš„ä½¿ç”¨
   - å­¦ä¼šäº†CPUã€å†…å­˜ã€Goroutineç­‰æ€§èƒ½æŒ‡æ ‡çš„åˆ†æ
   - ç†è§£äº†æ€§èƒ½ç“¶é¢ˆçš„è¯†åˆ«å’Œå®šä½æ–¹æ³•

2. **å†…å­˜ä¼˜åŒ–ç­–ç•¥** ğŸ§ 
   - æ·±å…¥äº†è§£äº†å†…å­˜æ³„æ¼çš„æ£€æµ‹å’Œé¢„é˜²
   - æŒæ¡äº†GCè°ƒä¼˜çš„æ–¹æ³•å’ŒæŠ€å·§
   - å­¦ä¼šäº†å¯¹è±¡æ± çš„è®¾è®¡å’Œä½¿ç”¨

3. **å¹¶å‘æ€§èƒ½è°ƒä¼˜** âš¡
   - æŒæ¡äº†Goroutineæ± çš„è®¾è®¡å’Œä¼˜åŒ–
   - å­¦ä¼šäº†Channelçš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§
   - ç†è§£äº†é”ä¼˜åŒ–å’Œæ— é”ç¼–ç¨‹

4. **æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–** ğŸ—„ï¸
   - æŒæ¡äº†è¿æ¥æ± çš„é…ç½®å’Œç›‘æ§
   - å­¦ä¼šäº†SQLæŸ¥è¯¢çš„ä¼˜åŒ–ç­–ç•¥
   - ç†è§£äº†æ•°æ®åº“æ€§èƒ½ç›‘æ§çš„é‡è¦æ€§

5. **ç¼“å­˜ç­–ç•¥ä¼˜åŒ–** ğŸ’¾
   - æ·±å…¥äº†è§£äº†å¤šçº§ç¼“å­˜çš„è®¾è®¡åŸç†
   - æŒæ¡äº†ç¼“å­˜ç­–ç•¥çš„é€‰æ‹©å’Œå®ç°
   - å­¦ä¼šäº†ç¼“å­˜æ€§èƒ½çš„ç›‘æ§å’Œä¼˜åŒ–

6. **ç½‘ç»œæ€§èƒ½ä¼˜åŒ–** ğŸŒ
   - æŒæ¡äº†HTTP/2å’Œè¿æ¥å¤ç”¨çš„ä¼˜åŒ–
   - å­¦ä¼šäº†å‹ç¼©å’Œç¼“å­˜çš„ç½‘ç»œä¼˜åŒ–æŠ€æœ¯
   - ç†è§£äº†ç½‘ç»œæ€§èƒ½ç›‘æ§çš„æ–¹æ³•

### ğŸš€ æŠ€æœ¯è¿›é˜¶

- **å¾®åŸºå‡†æµ‹è¯•**: ä½¿ç”¨go test -benchè¿›è¡Œç²¾ç¡®çš„æ€§èƒ½æµ‹è¯•
- **ç«ç„°å›¾åˆ†æ**: ä½¿ç”¨ç«ç„°å›¾å¯è§†åŒ–æ€§èƒ½ç“¶é¢ˆ
- **åˆ†å¸ƒå¼æ€§èƒ½ä¼˜åŒ–**: è·¨æœåŠ¡çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- **äº‘åŸç”Ÿæ€§èƒ½ä¼˜åŒ–**: å®¹å™¨åŒ–ç¯å¢ƒä¸‹çš„æ€§èƒ½è°ƒä¼˜

### ğŸ’¡ æœ€ä½³å®è·µ

1. **æµ‹é‡é©±åŠ¨ä¼˜åŒ–**: å…ˆæµ‹é‡ï¼Œå†ä¼˜åŒ–ï¼Œé¿å…è¿‡æ—©ä¼˜åŒ–
2. **æ¸è¿›å¼ä¼˜åŒ–**: ä»æœ€å¤§çš„ç“¶é¢ˆå¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–
3. **æŒç»­ç›‘æ§**: å»ºç«‹æ€§èƒ½ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
4. **å¹³è¡¡æƒè¡¡**: åœ¨æ€§èƒ½ã€å¯ç»´æŠ¤æ€§ã€å¼€å‘æ•ˆç‡é—´æ‰¾å¹³è¡¡

æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ç»“åˆå…·ä½“çš„ä¸šåŠ¡åœºæ™¯å’Œç³»ç»Ÿç‰¹ç‚¹ã€‚é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»å…·å¤‡äº†ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–èƒ½åŠ›ï¼ ğŸ‰

---

*è‡³æ­¤ï¼ŒGoè¯­è¨€å­¦ä¹ æ–‡æ¡£ç³»åˆ—çš„é«˜çº§ç¯‡å…¨éƒ¨å®Œæˆï¼ä»ç”Ÿäº§å®è·µåˆ°å®¹å™¨åŒ–éƒ¨ç½²ï¼Œä»ç›‘æ§æ—¥å¿—åˆ°æ€§èƒ½ä¼˜åŒ–ï¼Œæˆ‘ä»¬æ„å»ºäº†å®Œæ•´çš„ä¼ä¸šçº§Goå¼€å‘æŠ€èƒ½ä½“ç³»ï¼* ğŸš€

**ğŸŠ æ­å–œä½ å®Œæˆäº†æ•´ä¸ªGoè¯­è¨€å­¦ä¹ æ–‡æ¡£ç³»åˆ—ï¼ç°åœ¨ä½ å·²ç»å…·å¤‡äº†ä»å…¥é—¨åˆ°ç²¾é€šçš„å®Œæ•´Goå¼€å‘èƒ½åŠ›ï¼** ğŸŠ
